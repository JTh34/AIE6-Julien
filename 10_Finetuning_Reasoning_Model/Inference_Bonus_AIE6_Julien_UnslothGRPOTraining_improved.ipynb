{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPwFBEs1mvV"
      },
      "source": [
        "# Bonus Activity - Unsloth GRPO Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF4PsmQK0fiE"
      },
      "source": [
        "### 1. Insall Unsloth for Collab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ioBFUk9g1mvZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm==0.7.3\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm==0.7.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sUNfTmN3tY46"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UTAL2sPELqS",
        "outputId": "9b4caee4-8e27-4a7a-900b-60470eb8d87b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 05-06 13:21:45 __init__.py:207] Automatically detected platform cuda.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from vllm import SamplingParams\n",
        "from google.colab import drive\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from datasets import load_dataset, Dataset,load_from_disk\n",
        "import gc\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define system prompt used for training\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Solve the following mathematical problem step by step. Your response must follow this format:\n",
        "\n",
        "<reasoning>\n",
        "Provide a clear, step-by-step solution to the problem. Include all mathematical steps and logic.\n",
        "</reasoning>\n",
        "\n",
        "<answer>\n",
        "Write your final answer here concisely.\n",
        "</answer>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QG_B4dU1nP6U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the base model (no training)\n",
        "print(\"Loading base model...\")\n",
        "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    fast_inference = True,\n",
        ")\n",
        "print(\"Base model loaded successfully!\")"
      ],
      "metadata": {
        "id": "dHIxeWazjrZR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 934,
          "referenced_widgets": [
            "701c15fb78cd46929c5230d0050c417d",
            "4247c9c7bcd4490daa9ae825b80fa05b",
            "a9ddfbfd758745a8baf5b404f40bc776",
            "bbfa0e79eca240e49b7dff5f03c5d6c7",
            "6c4ced3c743b47dfb8552f300ad4c1cf",
            "a14ce33c465947188f94371574dbb8d5",
            "ec117dc3f3784e4fab46d7e360d03db9",
            "bc88f936d0e64a0c809e403c97fab0f7",
            "bfbb5f391c7842178e4c6b21684d2d67",
            "580708e5326847aaa688fc8b5a7806d8",
            "f060d01db4874680a17d02917f68ef63",
            "14df42447e5f43a9ad4c3031400b51bf",
            "4604131655d9434aa0a95a92e757d1c3",
            "5191b017aad948e4a2566246b9fc2c2a",
            "893c932861bf4438bb63a1edcb55220e",
            "fd22603532a549d8aaa0c4fc3fba4391",
            "7c8ecbdba2da4225b35dc99b36e2ffdc",
            "681af632dc374790a7bddb4a09fe841b",
            "ecff724d54de41c8bddef0e89caee257",
            "964e3065d96f472e9404497c71ac876b",
            "5328fff8bcf948fab48aec419bdbadec",
            "63a677866b334b19ab73bd8c0bbc5c7a",
            "cf4812c3dfb0497c88ff99598056fd0b",
            "1d36b3a551f64c9998d6598d4057ef4d",
            "3af09a0ca0e4426db7c522324f1e8c1f",
            "1ba4fb22e01b4615b04de710faeba476",
            "2c1d2d3f05d640928dc16b09511b341a",
            "ef78aaae04b547ddb7bc4322e0de98c6",
            "b4d7a59e89d6460e980b5a00d0348ece",
            "111d4bc18e02437880d3a6e4a7bbb5f8",
            "32772a4e293946f5a1da568a7e7c99dc",
            "0739f84c63c843f790985211232f1612",
            "e93abec33a8a4aee840ba20c7ef78900",
            "10795730f67f4198b26b17ed11cd85ce",
            "99f45e03a4df4e3bb697f0bb1a8f4f35",
            "a05f6de2513d408dae3a21b66a146ffe",
            "4e841b4b6cae486ba04baa287e167812",
            "58832b02dd584752b29934f49d671102",
            "fcae05225f6e49b89a00936aab4f3528",
            "536d84f89ad14b838513bd8fa4dd306a",
            "e86c121a66e14c8589ac9fa988c2a754",
            "1968d9f467624e56827098aa990278bd",
            "8fafec4f29874e93b026a5d587eaffdf",
            "24bf4c8895394f3fb58af104b279a557",
            "68ee5751b720441c985ec5f91252ab30",
            "51333f9e8fb94c08867ffc5f0e426893",
            "42d132c05ea142bf8da386f205cc0228",
            "3b32030c2b2f42369fd07014676c67a9",
            "29969c603b1b434880d1adefcce1bc36",
            "bb12dd934de74de9bde2d6b978eb1e24",
            "5e83637458e144de85c72526e3fdd8ba",
            "55dd9e33844f4fcfae81dffc9b2265c4",
            "5fb6c9ed04824bd48c6a85080f3c340a",
            "db8c93e6ba694bc8a203d5f75b06ed05",
            "1ee1e25a013944ad9b689b2dab1abe30",
            "cfc334c98eaa4374ba4d3a32be27f716",
            "6c777683a3944a7289d59a43b4cf418d",
            "9986b492aeea4809b1b6dd0b8f5908a2",
            "d7544134819f4a899331d519058fcebc",
            "cf32e48efe8b4893a5eaf6068690aa53",
            "7fdc20efb408409e87523ea6b2ff2e6b",
            "02c2d3a8a73747e1847502a8aaf35748",
            "4c74712e732c4f0291688cec099eea9f",
            "ac2370f7806f48259af09cd14edb29ac",
            "7f90ed48da2847409ec2d891057b3cda",
            "f27cbb18f7dd48e4bf611e0670705988",
            "133c549fde49401886be5104ec55e83c",
            "906302cf9ebb41b9a48ac25d9d0e162e",
            "88a0388f3a65481b82487036923fd6d3",
            "b1d003b85a7a4ad491fa1d1b57c2c02b",
            "c4cb36bf56784d1eb2afb8330497a96b",
            "8d64b8f0d1a64a768346e5a3e64b654b",
            "f08e8bc81970445aaa1114727120b710",
            "5409d6b92f90448f8c75a515d31984a7",
            "13dcb31b5d1a4eef8e09dfe8fb4e208f",
            "9b6fdd83f9bf419483303ff48c23ef69",
            "4af186c96d2740f08298ddb53c81a87a",
            "82fdc448289642d2b187e349e3e1e541",
            "48892d3bd0c94ee6868e5b187a5c6444",
            "83c7245de6b9454db2c8945e498fc02e",
            "f90c5cd64f0c411b90b1905844b96a58",
            "a066138bbbc0442db959b16cdae66a1c",
            "97c577cd1a794bb8ae0e520cdc64fb0c",
            "1e86df4d63db4ed581b165c9c1e93a84",
            "34ada7b9acd34cd48974264544afc81f",
            "ad1c3aac5daa456d822e8ffefb033348",
            "588c8e37ed8a4bac85a4f906b4fde3e1",
            "32da9d310ace41deaa76953b9ae7df48",
            "b5ad1d947f0e4692ba89523d2da0421c",
            "e783149c0f7d4c419b69ac0d4faca267",
            "10feb2b88e1445d9a19744d09fef7647",
            "78d9f0df3dbd4ef3ac7cf512d295bc2b",
            "21092f8125a747bda23b98aa045e4475",
            "7167a027602d449a9a492762c12ef549",
            "baeaa71b82c241ceb5f56b4609242005",
            "a2f4e8e6eb8b4a2ebceadad91a940036",
            "f141e6f159e94446a862b691ea34b75b",
            "894ac518495b4c7a85daa06e0a0562ee",
            "e492178274924b90bd8eddf6a42fd84f",
            "32fd5a8d904f498fb6272dd900e06fe0",
            "349eb80cd7ac437598d2133418a74621",
            "37621a829882488786d321d0202cdcf5",
            "4afdd1d0221c4abdb1f49b180233452a",
            "b72d61bc810b47ac86c57473562e3daf",
            "6d4185f902e0446896991a3cc3fdbdb7",
            "7d15dc102bae4cf68c21d6563a809ceb",
            "5da59556c0b14d3e887f76ffec6f71b0",
            "01de802f6e0142cf8e14aff6265657a9",
            "175af251166146b28dc02eb7e166d781",
            "1318073e7c724d10b8974748a1ad5093"
          ]
        },
        "outputId": "31077a54-5bb2-4c02-e2f6-61ec9d105277"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model...\n",
            "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.7.3.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 49.49%\n",
            "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 22.16 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 224.\n",
            "Unsloth: vLLM's KV Cache can use up to 8.41 GB. Also swap space = 6 GB.\n",
            "INFO 05-06 13:25:19 config.py:549] This model supports multiple tasks: {'embed', 'generate', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 05-06 13:25:19 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":224}, use_cached_outputs=False, \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "701c15fb78cd46929c5230d0050c417d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14df42447e5f43a9ad4c3031400b51bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf4812c3dfb0497c88ff99598056fd0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10795730f67f4198b26b17ed11cd85ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 05-06 13:25:26 cuda.py:229] Using Flash Attention backend.\n",
            "INFO 05-06 13:25:26 model_runner.py:1110] Starting to load model unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit...\n",
            "INFO 05-06 13:25:27 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
            "INFO 05-06 13:25:29 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68ee5751b720441c985ec5f91252ab30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 05-06 13:26:07 weight_utils.py:270] Time spent downloading weights for unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit: 38.191973 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cfc334c98eaa4374ba4d3a32be27f716"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "133c549fde49401886be5104ec55e83c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 05-06 13:26:09 model_runner.py:1115] Loading model weights took 2.2405 GB\n",
            "INFO 05-06 13:26:09 punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 05-06 13:26:18 worker.py:267] Memory profiling takes 8.84 seconds\n",
            "INFO 05-06 13:26:18 worker.py:267] the current vLLM instance can use total_gpu_memory (22.16GiB) x gpu_memory_utilization (0.49) = 10.97GiB\n",
            "INFO 05-06 13:26:18 worker.py:267] model weights take 2.24GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.05GiB; the rest of the memory reserved for KV Cache is 7.64GiB.\n",
            "INFO 05-06 13:26:19 executor_base.py:111] # cuda blocks: 4469, # CPU blocks: 3510\n",
            "INFO 05-06 13:26:19 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 34.91x\n",
            "INFO 05-06 13:26:22 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:42<00:00,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 05-06 13:27:05 model_runner.py:1562] Graph capturing finished in 43 secs, took 0.57 GiB\n",
            "INFO 05-06 13:27:05 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 55.71 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82fdc448289642d2b187e349e3e1e541"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5ad1d947f0e4692ba89523d2da0421c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32fd5a8d904f498fb6272dd900e06fe0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GRPO-trained LoRA model separately\n",
        "print(\"Loading GRPO-trained model...\")\n",
        "grpo_model, _ = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    fast_inference = True,\n",
        "    lora_path = \"/content/drive/MyDrive/math_reasoning_grpo_lora_improved\"\n",
        ")\n",
        "print(\"GRPO-trained model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622,
          "referenced_widgets": [
            "150a56a5882548ddb8c3c82e68fe6c52",
            "a57050e66c7b4cd5a816232d4c4e8069",
            "dec6180c66834669894df36287584b02",
            "5d07c88211614975a13f5d2074c51e7b",
            "c73c8b86f3834373ace24fb97c0e8a53",
            "f62aca3836164b49b318c290960cedbf",
            "28a6c79fdbdc47779000848979ca2013",
            "809638684f7545f1b00706aac06b7b79",
            "3c3855bafbb14394a7ba67c59781b08a",
            "8e40929268bd4fbe858a543888e862a4",
            "a84b6ac542994b688cd797b10a772f80",
            "26a3a2257c4e4e60bc598474654689eb",
            "9adbff212b5d416eb56b8b3e49e936fa",
            "7303b4792abf463d95dec46d791c03cd",
            "e08588b141034f27a689140249c1b6a1",
            "e478087e5bf34905b99c84f46a53b4be",
            "4edb3a0706264cc5a8af524d6053d5f4",
            "a52c898fb78f435fbac3710f0b92c622",
            "ab9ddec00a394f449e5c95c33eb10493",
            "d001c1410d4c4ebc82a9e9120f867145",
            "4f209da40d10474795ff1f57270d1fee",
            "d1af57a08591413f83f45932f91f0c78"
          ]
        },
        "id": "O0PpTpRIwL19",
        "outputId": "2a3ad063-c2bb-4b54-f5f6-d227a0f42199"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GRPO-trained model...\n",
            "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.7.3.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 25.18%\n",
            "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 22.16 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 160.\n",
            "Unsloth: vLLM's KV Cache can use up to 3.02 GB. Also swap space = 5 GB.\n",
            "INFO 05-06 13:27:25 config.py:549] This model supports multiple tasks: {'embed', 'generate', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 05-06 13:27:25 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":160}, use_cached_outputs=False, \n",
            "INFO 05-06 13:27:27 model_runner.py:1110] Starting to load model unsloth/llama-3.2-3b-instruct-unsloth-bnb-4bit...\n",
            "INFO 05-06 13:27:27 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
            "INFO 05-06 13:27:28 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "150a56a5882548ddb8c3c82e68fe6c52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26a3a2257c4e4e60bc598474654689eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 05-06 13:27:30 model_runner.py:1115] Loading model weights took 2.2093 GB\n",
            "INFO 05-06 13:27:32 worker.py:267] Memory profiling takes 0.98 seconds\n",
            "INFO 05-06 13:27:32 worker.py:267] the current vLLM instance can use total_gpu_memory (22.16GiB) x gpu_memory_utilization (0.25) = 5.58GiB\n",
            "INFO 05-06 13:27:32 worker.py:267] model weights take 2.21GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.74GiB; the rest of the memory reserved for KV Cache is 2.63GiB.\n",
            "INFO 05-06 13:27:32 executor_base.py:111] # cuda blocks: 1539, # CPU blocks: 2925\n",
            "INFO 05-06 13:27:32 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 12.02x\n",
            "INFO 05-06 13:27:36 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:32<00:00,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 05-06 13:28:08 model_runner.py:1562] Graph capturing finished in 33 secs, took 0.45 GiB\n",
            "INFO 05-06 13:28:08 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 37.96 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRPO-trained model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate response with base model\n",
        "def generate_response(prompt, lora=False):\n",
        "    \"\"\"Generate a response using either the base model or GRPO-trained model.\"\"\"\n",
        "\n",
        "    # Prepare input for inference\n",
        "    llm_prompt = [\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : prompt}\n",
        "    ]\n",
        "\n",
        "    text = base_tokenizer.apply_chat_template(llm_prompt, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Set generation parameters\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.8,\n",
        "        top_p=0.95,\n",
        "        max_tokens=1024,\n",
        "    )\n",
        "\n",
        "    # Choose which model to use based on the lora parameter\n",
        "    model = grpo_model if lora else base_model\n",
        "\n",
        "    # Generate response\n",
        "    output = model.fast_generate(\n",
        "        [text],\n",
        "        sampling_params=sampling_params,\n",
        "    )[0].outputs[0].text\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "pJQ8Yrr6sHC5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test dataset\n",
        "test_dataset = load_from_disk(\"/content/drive/MyDrive/test_math_dataset_improved\")\n",
        "\n",
        "# Select 3 random examples from the test dataset\n",
        "random.seed(123)  # For reproducibility\n",
        "sample_indices = random.sample(range(len(test_dataset)), 4)\n",
        "sample_examples = [test_dataset[i] for i in sample_indices]\n",
        "\n",
        "print(f\"Selected example indices: {sample_indices}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm4VRxN5rXM9",
        "outputId": "84d2274c-04ea-4ecb-efa5-cf937a34db68"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected example indices: [3, 17, 5, 49]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate each example\n",
        "for i, example in enumerate(sample_examples):\n",
        "    print(f\"\\n{'='*40} Example #{i+1} {'='*40}\")\n",
        "    print(f\"\\nProblem:\\n{example['prompt'][1]['content']}\")\n",
        "    print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
        "\n",
        "    print(f\"Expected answer:\\n{example['expected_answer']}\")\n",
        "    print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
        "\n",
        "    print(\"Base model (no training):\")\n",
        "    base_response = generate_response(prompt = example['prompt'] ,lora= False)\n",
        "    print(f\"{base_response}\")\n",
        "    print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
        "\n",
        "    print(\"GRPO-trained model:\")\n",
        "    grpo_response = generate_response(example['prompt'],lora= True)\n",
        "    print(f\"{grpo_response}\")\n",
        "    print(\"\\n\" + \"=\" * 80 + \"\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c4y_figuGm_",
        "outputId": "2fc31873-7536-40fe-83a4-014d32b30b5e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================== Example #1 ========================================\n",
            "\n",
            "Problem:\n",
            "Given circle A: $x^2+y^2=1$, and circle B: $(x-3)^2+(y+4)^2=10$. Let P be a moving point in the plane, and tangents to circle A and circle B are drawn from P, touching the circles at points D and E, respectively. If $PE=PD$, then the minimum distance from P to the origin is \\_\\_\\_\\_\\_.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Expected answer:\n",
            "\\frac{8}{5}\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Base model (no training):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.25s/it, est. speed input: 53.75 toks/s, output: 65.56 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<reasoning>\n",
            "\n",
            "To solve this problem, we can start by defining the coordinates of points D and E. We know that the tangent to a circle is perpendicular to the radius drawn to the point of tangency. Let's define the coordinates of point D as (x, y) and the coordinates of point E as (x, y + 2sqrt(9 - 10)).\n",
            "\n",
            "Since PE = PD, we can write the equation of line DE as y = -sqrt(9 - 10)(x - 0) + (0 + sqrt(9 - 10)) = -sqrt(1)(x) + sqrt(1) = -x + 1.\n",
            "\n",
            "Next, we can find the equation of the tangent to circle A (x^2 + y^2 = 1) as y = -x + 1 and the equation of the tangent to circle B ((x - 3)^2 + (y + 4)^2 = 10) as y = -(x - 3) + 7.\n",
            "\n",
            "We can equate these two equations to find the x-coordinate of point P: -x + 1 = -(x - 3) + 7. Solving for x, we get x = 1. Substituting this value back into either of the tangent equations, we get y = 0.\n",
            "\n",
            "Next, we can use the distance formula to find the distance from P to the origin: d = sqrt((1 - 0)^2 + (0 - 0)^2) = sqrt(1) = 1.\n",
            "\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "\n",
            "The minimum distance from point P to the origin is 1.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "GRPO-trained model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.13s/it, est. speed input: 23.25 toks/s, output: 65.72 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<reasoning>\n",
            "To solve this problem, we first need to understand the geometry involved. We have two circles, A and B, with equations $x^2 + y^2 = 1$ and $(x - 3)^2 + (y + 4)^2 = 10$, respectively. We're looking for the minimum distance from a point P to the origin, given that the tangents from P to the circles A and B have equal length.\n",
            "\n",
            "Since the circles are given in standard form, we can find their centers and radii. Circle A has center (0, 0) and radius 1, while circle B has center (3, -4) and radius $\\sqrt{10}$.\n",
            "\n",
            "Given that the tangents from P to the circles are equal in length, we can use the fact that the lengths of tangents from an external point to a circle are equal. Therefore, $PD = PE = d$, where $d$ is the distance from P to the origin.\n",
            "\n",
            "To minimize the distance from P to the origin, we can use the triangle inequality: $d \\geq |PE| - |PD|$.\n",
            "\n",
            "Since $PE = PD$, we have $d \\geq 0$.\n",
            "\n",
            "Now, let's consider the geometry. We can draw a line from the center of circle B to the origin. This line intersects the x-axis at $x = 3$, and the y-axis at $y = -4$. We can use the Pythagorean theorem to find the distance from the origin to the center of circle B: $\\sqrt{3^2 + (-4)^2} = \\sqrt{25} = 5$.\n",
            "\n",
            "The distance from the center of circle B to P is $d = |PD| + |PE| = |PE| + |PD|$. Since $d \\geq 0$, we can write $d \\geq |PD| + |PE| \\geq |PE| + |PE| = 2|PE|$. We know that $d \\geq 0$, so $d \\geq 0$.\n",
            "\n",
            "Next, we consider the line that passes through the center of circle A and the point P. This line intersects the x-axis at $x = 0$, and the y-axis at $y = 0$. Using the Pythagorean theorem again, we can find the distance from the origin to the center of circle A: $\\sqrt{0^2 + 0^2} = 0$.\n",
            "\n",
            "Now, let's analyze the triangle formed by the origin, the center of circle A, and the center of circle B. Since the radii of the circles are 1 and $\\sqrt{10}$, respectively, the length of the base of the triangle is $\\sqrt{10} - 1$. The height of the triangle is the distance from the origin to the center of circle A, which is 0.\n",
            "\n",
            "Using the Pythagorean theorem, we can find the length of the hypotenuse of the triangle: $\\sqrt{(\\sqrt{10} - 1)^2 + 0^2} = \\sqrt{9 + 2\\sqrt{10} - 1} = \\sqrt{8 + 2\\sqrt{10}} = \\sqrt{4(2 + \\sqrt{10})} = 2\\sqrt{2 + \\sqrt{10}}$.\n",
            "\n",
            "Since the distance from the origin to the point P is at least as great as the length of the hypotenuse, we have $d \\geq 2\\sqrt{2 + \\sqrt{10}}$.\n",
            "\n",
            "Therefore, the minimum distance from P to the origin is $2\\sqrt{2 + \\sqrt{10}}$.\n",
            "\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "$2\\sqrt{2 + \\sqrt{10}}$\n",
            "</answer>\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "\n",
            "======================================== Example #2 ========================================\n",
            "\n",
            "Problem:\n",
            "Given vectors $\\overrightarrow{a}=(-1,2)$ and $\\overrightarrow{b}=(1,-2y)$, find the value of $y$ if $\\overrightarrow{a}$ is parallel to $\\overrightarrow{b}$.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Expected answer:\n",
            "1\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Base model (no training):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.37s/it, est. speed input: 55.61 toks/s, output: 65.22 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<reasoning>\n",
            "To determine if two vectors are parallel, we need to check if one vector is a scalar multiple of the other. In this case, we need to find the value of $y$ such that $\\overrightarrow{a}$ is parallel to $\\overrightarrow{b}$. We can write this as $\\overrightarrow{a} = k\\overrightarrow{b}$, where $k$ is a scalar.\n",
            "\n",
            "We can write the equation $\\overrightarrow{a} = k\\overrightarrow{b}$ as:\n",
            "\n",
            "$(-1, 2) = k(1, -2y)$\n",
            "\n",
            "We can equate the corresponding components to get:\n",
            "\n",
            "$-1 = k$ and $2 = -2ky$\n",
            "\n",
            "Now we can solve for $y$:\n",
            "\n",
            "$2 = -2ky$\n",
            "\n",
            "$2 = -2k(y)$\n",
            "\n",
            "Dividing both sides by -2:\n",
            "\n",
            "$-1 = ky$\n",
            "\n",
            "Dividing both sides by $k$ (we can ignore $k$ since we already know it's equal to -1):\n",
            "\n",
            "$-1/y = -1$\n",
            "\n",
            "Now we can solve for $y$:\n",
            "\n",
            "$y = 1$\n",
            "\n",
            "Therefore, $y$ must be equal to 1 in order for $\\overrightarrow{a}$ to be parallel to $\\overrightarrow{b}$.\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "$y = 1$\n",
            "</answer>\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "GRPO-trained model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.79s/it, est. speed input: 64.21 toks/s, output: 65.27 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<reasoning>\n",
            "To find the value of $y$ if vectors $\\overrightarrow{a}$ and $\\overrightarrow{b}$ are parallel, we need to use the property that two vectors are parallel if they are scalar multiples of each other. \n",
            "\n",
            "We know that $\\overrightarrow{a}=(-1,2)$ and $\\overrightarrow{b}=(1,-2y)$. If these vectors are parallel, there exists some scalar $k$ such that $\\overrightarrow{b} = k\\overrightarrow{a}$. We can set up the following system of equations using the components of the vectors:\n",
            "\\begin{align*}\n",
            "1 &= k(-1)\\\\\n",
            "-2y &= k(2)\n",
            "\\end{align*}\n",
            "We can solve the first equation to find that $k = -1$. Then, we can substitute this value of $k$ into the second equation:\n",
            "\\begin{align*}\n",
            "-2y &= -1(2)\\\\\n",
            "-2y &= -2\\\\\n",
            "y &= 1\n",
            "\\end{align*}\n",
            "Therefore, the value of $y$ is 1.\n",
            "\n",
            "\\end{reasoning}\n",
            "\n",
            "<answer>\n",
            "$y=1$\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "\n",
            "======================================== Example #3 ========================================\n",
            "\n",
            "Problem:\n",
            "The area of the closed region bounded by $xy=1$, $y=x$, and $x=3$ is (ã€€ã€€)\n",
            "A: $2\\ln3$\n",
            "B: $2+\\ln3$\n",
            "C: $4-2\\ln3$\n",
            "D: $4-\\ln3$\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Expected answer:\n",
            "D\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Base model (no training):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.87s/it, est. speed input: 37.72 toks/s, output: 65.25 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<reasoning>\n",
            "To solve this problem, we will use the concept of integration to find the area of the closed region bounded by the curves $xy=1$, $y=x$, and $x=3$.\n",
            "First, we need to find the intersection points of these curves. We can rewrite the equation $xy=1$ as $y=\\frac{1}{x}$. The other two curves are given as $y=x$ and $x=3$.\n",
            "We can see that the curve $y=\\frac{1}{x}$ intersects with $y=x$ at $x=1$ since $1 \\cdot 1 = 1$. This means that the region bounded by these curves is the area between $x=1$ and $x=3$, between the curves $y=\\frac{1}{x}$ and $y=x$.\n",
            "Next, we can set up an integral to represent the area of the region.\n",
            "$A = \\int_{1}^{3} (x-\\frac{1}{x}) dx$\n",
            "Now, we will integrate the function with respect to $x$.\n",
            "$A = \\int_{1}^{3} (x-\\frac{1}{x}) dx = [\\frac{x^{2}}{2} - \\ln(x)] \\bigg|_{1}^{3}$\n",
            "Using the fundamental theorem of calculus, we can evaluate the definite integral as follows:\n",
            "$A = [\\frac{x^{2}}{2} - \\ln(x)] \\bigg|_{1}^{3} = (\\frac{3^{2}}{2} - \\ln(3)) - (\\frac{1^{2}}{2} - \\ln(1))$\n",
            "$A = (\\frac{9}{2} - \\ln(3)) - \\frac{1}{2} + 0$\n",
            "$A = 4 - \\ln(3)$\n",
            "Therefore, the area of the closed region bounded by $xy=1$, $y=x$, and $x=3$ is $4 - \\ln(3)$.\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "$4 - \\ln(3)$\n",
            "</answer>\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "GRPO-trained model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.72s/it, est. speed input: 33.57 toks/s, output: 65.58 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<reasoning>\n",
            "To solve this problem, we first need to visualize the region bounded by the curves $xy=1$, $y=x$, and $x=3$. \n",
            "\n",
            "We can rewrite the equation $xy=1$ as $y = \\frac{1}{x}$. This represents a hyperbola with asymptotes at $x=0$ and $y=0$. \n",
            "\n",
            "We can also graph the line $x=3$, which is a vertical line passing through the point (3, 0).\n",
            "\n",
            "The region of interest is the closed area between the hyperbola $y = \\frac{1}{x}$, the line $y=x$, and the line $x=3$.\n",
            "\n",
            "To find the area of this region, we can use integration.\n",
            "\n",
            "Let's start by finding the intersection points of the hyperbola and the line $y=x$. Setting $y = \\frac{1}{x}$ equal to $y=x$, we can solve for $x$:\n",
            "\n",
            "$\\frac{1}{x} = x$\n",
            "\n",
            "$x^2 = 1$\n",
            "\n",
            "$x = 1$ (since $x$ cannot be negative in this case)\n",
            "\n",
            "Now, we need to integrate the area between the curve $y = \\frac{1}{x}$ and the line $y=x$ from $x=1$ to $x=3$.\n",
            "\n",
            "The area of this region is given by:\n",
            "\n",
            "$A = \\int_1^3 \\left( x - \\frac{1}{x} \\right) dx$\n",
            "\n",
            "$A = \\int_1^3 \\left( x - \\frac{1}{x} \\right) dx$\n",
            "\n",
            "$A = \\int_1^3 x dx - \\int_1^3 \\frac{1}{x} dx$\n",
            "\n",
            "$A = \\left[ \\frac{x^2}{2} \\right]_1^3 - \\left[ \\ln|x| \\right]_1^3$\n",
            "\n",
            "$A = \\left( \\frac{3^2}{2} - \\frac{1^2}{2} \\right) - (\\ln|3| - \\ln|1|)$\n",
            "\n",
            "$A = \\left( \\frac{9}{2} - \\frac{1}{2} \\right) - (\\ln3 - 0)$\n",
            "\n",
            "$A = 4 - \\ln3$\n",
            "\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "$4 - \\ln3$\n",
            "</answer>\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "\n",
            "======================================== Example #4 ========================================\n",
            "\n",
            "Problem:\n",
            "In a cave, there was a pile of peaches that belonged to three monkeys. The eldest monkey came to the cave and divided the peaches into two parts in a 5:4 ratio, taking the larger portion for himself. The second monkey arrived later and divided the remaining peaches again in a 5:4 ratio, taking the larger portion for himself. The remaining peaches were left for the third monkey. It is known that the eldest monkey took 29 more peaches than the third monkey. How many peaches did the second monkey take?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Expected answer:\n",
            "20\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Base model (no training):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.80s/it, est. speed input: 44.10 toks/s, output: 65.42 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<reasoning>\n",
            "To solve this problem, let's denote the total number of peaches as 'x'. \n",
            "\n",
            "The eldest monkey divides the peaches in a 5:4 ratio, taking the larger portion for himself. Therefore, the eldest monkey takes 5/9 of the total peaches.\n",
            "\n",
            "Let the number of peaches the third monkey gets be 'y'. After the first division, the remaining peaches for the first two monkeys will be (4/9)x.\n",
            "\n",
            "The second monkey then divides these remaining peaches in a 5:4 ratio, taking the larger portion for himself. Therefore, the second monkey takes 5/9 of the remaining peaches.\n",
            "\n",
            "We know that the eldest monkey took 29 more peaches than the third monkey. So, we can set up the equation:\n",
            "\n",
            "5/9x - y = 29\n",
            "\n",
            "Since the second monkey took 5/9 of the remaining peaches after the first division, we can also write the equation:\n",
            "\n",
            "5/9 * (4/9)x = 5/9y\n",
            "\n",
            "We can simplify the second equation to get:\n",
            "\n",
            "20/81x = 5/9y\n",
            "\n",
            "Now we can substitute y in the first equation:\n",
            "\n",
            "5/9x - 20/81x = 29\n",
            "\n",
            "Multiplying the whole equation by 81 to eliminate the fraction:\n",
            "\n",
            "45x - 20x = 2343\n",
            "\n",
            "25x = 2343\n",
            "\n",
            "Dividing both sides by 25:\n",
            "\n",
            "x = 93.92\n",
            "\n",
            "Now, we can find the number of peaches the third monkey gets:\n",
            "\n",
            "y = 5/9 * (4/9)x\n",
            "y = 5/81 * 93.92\n",
            "y = 5 * 10.48\n",
            "y = 52.4\n",
            "\n",
            "However, since we cannot have a fraction of a peach, the number of peaches must be an integer. The issue arises because the problem does not provide enough information to find the exact number of peaches.\n",
            "\n",
            "But if we follow the problem's instruction for the given format to the letter, I'll provide the answer as a decimal value, despite the issue.\n",
            "\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "52.4\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "GRPO-trained model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.25s/it, est. speed input: 36.39 toks/s, output: 65.86 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<reasoning>\n",
            "To solve this problem, we need to first determine the number of peaches each monkey took. Let's assume the total number of peaches initially was x.\n",
            "\n",
            "The eldest monkey divided the peaches into two parts in a 5:4 ratio. Since the eldest monkey took the larger portion, he took 5/9 of the total peaches. So, the number of peaches the eldest monkey took is (5/9)x.\n",
            "\n",
            "The remaining peaches after the eldest monkey took his share is (4/9)x.\n",
            "\n",
            "The second monkey then divided these remaining peaches into two parts in a 5:4 ratio. The second monkey took the larger portion, which is 5/9 of the remaining peaches. Let's call the number of peaches the second monkey took y. So, we have (5/9)(4/9)x = y.\n",
            "\n",
            "The remaining peaches after the second monkey took his share is (4/9)(4/9)x = (16/81)x.\n",
            "\n",
            "Since the third monkey took the remaining peaches, the number of peaches the third monkey took is (16/81)x.\n",
            "\n",
            "According to the problem, the eldest monkey took 29 more peaches than the third monkey. So, we can set up the equation:\n",
            "\n",
            "(5/9)x - (16/81)x = 29\n",
            "\n",
            "We can simplify the equation:\n",
            "\n",
            "(45/81)x - (16/81)x = 29\n",
            "(29/81)x = 29\n",
            "\n",
            "Now, let's solve for x:\n",
            "\n",
            "x = 29 * (81/29)\n",
            "x = 81\n",
            "\n",
            "Now that we know the total number of peaches, we can find the number of peaches the second monkey took. We know that the second monkey took 5/9 of the remaining peaches after the eldest monkey took his share. We can find the number of peaches the eldest monkey took first:\n",
            "\n",
            "Eldest monkey's share = (5/9) * 81 = 45\n",
            "\n",
            "The remaining peaches after the eldest monkey took his share is 81 - 45 = 36.\n",
            "\n",
            "Now we know that 36 peaches is 4/9 of the total. We can find y (the number of peaches the second monkey took) using the equation:\n",
            "\n",
            "y = (5/9) * 36\n",
            "y = (5/9) * (4/9) * 81\n",
            "y = (20/81) * 81\n",
            "y = 20\n",
            "\n",
            "So, the second monkey took 20 peaches.\n",
            "</reasoning>\n",
            "\n",
            "<answer>\n",
            "The second monkey took 20 peaches.\n",
            "</answer>\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BWnwa17Hu1tt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}