{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=”false” ><img src=\"https://github.com/AI-Maker-Space/LLM-Dev-101/assets/37101144/d1343317-fa2f-41e1-8af1-1dbb18399719\" \n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>\n",
    "\n",
    "<h1 align=\"center\" id=\"heading\">OpenAI Agents SDK - AIM</h1>\n",
    "\n",
    "In this notebook, we'll go over some of the key features of the OpenAI Agents SDK - as explored through a notebook-ified version of their [Research Bot](https://github.com/openai/openai-agents-python/tree/main/examples/research_bot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You don't need to run this cell if you're running this notebook locally. \n",
    "\n",
    "#!pip install -qU openai-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nest Async:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "As may be expected, the primary thing we'll do in the Agents SDK is construct Agents!\n",
    "\n",
    "Agents are constructed with a few basic properties:\n",
    "\n",
    "- A prompt, which OpenAI is using the language \"instruction\" for, that determines the behaviour or goal of the Agent\n",
    "- A model, the \"brain\" of the Agent\n",
    "\n",
    "They also typically include an additional property: \n",
    "\n",
    "- Tool(s) that equip the Agent with things it can use to get stuff done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Create Planner Agent\n",
    "\n",
    "Let's start by creating our \"Planner Agent\" - which will come up with the initial set of search terms that should answer a query provided by the user. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from agents import Agent\n",
    "\n",
    "PLANNER_PROMPT = (\n",
    "    \"You are a helpful research assistant. Given a query, come up with a set of web searches to perform\" \n",
    "    \"to best answer the query. Output between 5 and 20 terms to query for.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the data models that our Planner Agent will use to structure its output. We'll create:\n",
    "\n",
    "1. `WebSearchItem` - A model for individual search items, containing the search query and reasoning\n",
    "2. `WebSearchPlan` - A container model that holds a list of search items\n",
    "\n",
    "These Pydantic models will help ensure our agent returns structured data that we can easily process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebSearchItem(BaseModel):\n",
    "    reason: str\n",
    "    \"Your reasoning for why this search is important to the query.\"\n",
    "\n",
    "    query: str\n",
    "    \"The search term to use for the web search.\"\n",
    "\n",
    "class WebSearchPlan(BaseModel):\n",
    "    searches: list[WebSearchItem]\n",
    "    \"\"\"A list of web searches to perform to best answer the query.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our Planner Agent using the Agent class from the OpenAI Agents SDK. This agent will use the instructions defined in `PLANNER_PROMPT` and will output structured data in the form of our WebSearchPlan model. We're using the GPT-4o model for this agent to ensure high-quality search term generation.\n",
    "\n",
    "> NOTE: When we provide an `output_type` - the model will return a [structured response](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_agent = Agent(\n",
    "    name=\"PlannerAgent\",\n",
    "    instructions=PLANNER_PROMPT,\n",
    "    model=\"gpt-4.1\",\n",
    "    output_type=WebSearchPlan,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓Question #1:\n",
    "\n",
    "Why is it important to provide a structured response template? (As in: Why are structured outputs helpful/preferred in Agentic workflows?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 😎 ANSWER #1\n",
    "\n",
    "Sructured outputs allow to precisely define how information will be organized and transmitted between agents, improving the overall efficiency of the system and reducing the risk of errors or misunderstandings in data processing.\n",
    "\n",
    "It enables automated processing and effective collaboration in a multi-agent system. This structured schemas act as a safeguard, ensuring that agent actions and decisions align with predefined parameters, thus limiting unexpected behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Create Search Agent\n",
    "\n",
    "Now we'll create our Search Agent, which will be responsible for executing web searches based on the terms generated by the Planner Agent. This agent will take each search query, perform a web search using the `WebSearchTool`, and then summarize the results in a concise format.\n",
    "\n",
    "> NOTE: We are using the `WebSearchTool`, a hosted tool that can be used as part of an `OpenAIResponsesModel` as outlined in the [documentation](https://openai.github.io/openai-agents-python/tools/). This is based on the tools available through OpenAI's new [Responses API](https://openai.com/index/new-tools-for-building-agents/).\n",
    "\n",
    "The `SEARCH_PROMPT` below instructs the agent to create brief, focused summaries of search results. These summaries are designed to be 2-3 paragraphs, under 300 words, and capture only the essential information without unnecessary details. The goal is to provide the Writer Agent with clear, distilled information that can be efficiently synthesized into the final report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_PROMPT = (\n",
    "    \"You are a research assistant. Given a search term, you search the web for that term and\"\n",
    "    \"produce a concise summary of the results. The summary must 2-3 paragraphs and less than 300\"\n",
    "    \"words. Capture the main points. Write succinctly, no need to have complete sentences or good\"\n",
    "    \"grammar. This will be consumed by someone synthesizing a report, so its vital you capture the\"\n",
    "    \"essence and ignore any fluff. Do not include any additional commentary other than the summary\"\n",
    "    \"itself.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our Search Agent using the Agent class from the OpenAI Agents SDK. This agent will use the instructions defined in `SEARCH_PROMPT` and will utilize the `WebSearchTool` to perform web searches. We're configuring it with `tool_choice=\"required\"` to ensure it always uses the search tool when processing requests.\n",
    "\n",
    "> NOTE: We can, as demonstrated, indicate how we want our model to use tools. You can read more about that at the bottom of the page [here](https://openai.github.io/openai-agents-python/agents/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import WebSearchTool\n",
    "from agents.model_settings import ModelSettings\n",
    "\n",
    "search_agent = Agent(\n",
    "    name=\"Search agent\",\n",
    "    instructions=SEARCH_PROMPT,\n",
    "    tools=[WebSearchTool()],\n",
    "    model_settings=ModelSettings(tool_choice=\"required\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Question #2: \n",
    "\n",
    "What other tools are supported in OpenAI's Responses API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 😎 ANSWER #2 :\n",
    "There 2 kind of other tools :\n",
    "1. Basic Built-in Tools\n",
    "\n",
    "=> Web Search: for searching\n",
    "\n",
    "=> File Search: for searching previously uploaded files which allows :\n",
    "-  semantic and keyword searches in uploaded document\n",
    "- support for multiple file types\n",
    "- re-ranking of search results\n",
    "- filtering by attributes for more precise information retrieval\n",
    "- query rewriting\n",
    "\n",
    "=> Computer Usage Tool (CUA): for controlling computers or virtual machines, allowing the creation of agents capable of interacting with computer interfaces. This tool allows:\n",
    "- Control of computers or virtual machines\n",
    "- Interaction based on screenshots where the model recommends actions (click, scroll, type)\n",
    "\n",
    "2. Function Calls:\n",
    "The Responses API also supports function calls which allows:\n",
    "- Defining custom functions using the \"tools\" table\n",
    "- Model access to specific data or features not directly available\n",
    "- Integration with external systems for enhanced functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Create Writer Agent\n",
    "\n",
    "Finally, we'll create our Writer Agent, which will synthesize all the research findings into a comprehensive report. This agent takes the original query and the research summaries from the Search Agent, then produces a structured report with follow-up questions.\n",
    "\n",
    "The Writer Agent will:\n",
    "1. Create an outline for the report structure\n",
    "2. Generate a detailed markdown report (5-10 pages)\n",
    "3. Provide follow-up questions for further research\n",
    "\n",
    "We'll define the prompt for this agent in the next cell. This prompt will instruct the Writer Agent on how to synthesize research findings into a comprehensive report with follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_PROMPT = (\n",
    "    \"You are a senior researcher tasked with writing a cohesive report for a research query. \"\n",
    "    \"You will be provided with the original query, and some initial research done by a research \"\n",
    "    \"assistant.\\n\"\n",
    "    \"You should first come up with an outline for the report that describes the structure and \"\n",
    "    \"flow of the report. Then, generate the report and return that as your final output.\\n\"\n",
    "    \"The final output should be in markdown format, and it should be lengthy and detailed. Aim \"\n",
    "    \"for 5-10 pages of content, at least 1000 words.\\n\"\n",
    "    \"For the follow-up questions, provide exactly 5 unique questions that would help extend \"\n",
    "    \"this research. Do not repeat questions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🏗️ Activity #1: \n",
    "\n",
    "This prompt is quite generic - modify this prompt to produce a report that is more personalized to either your personal preference, or more appropriate for a specific use case (eg. law domain research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 😎 MY PROMPT\n",
    "WRITER_PROMPT = (\n",
    "\"You are an image and signal processing expert tasked with writing a comprehensive and detailed technical report on a research query. You will receive the original query as well as preliminary research conducted by an assistant.\"\n",
    "\"Your report should follow this structure:\"\n",
    "\"1. Begin by creating an introduction that clearly presents the problem and its importance in the field of image/signal processing.\"\n",
    "\"2. Develop a detailed outline that describes the structure of the report, including sections on:\"\n",
    "\" - The mathematical and technical foundations of the topic\"\n",
    "\" - Relevant algorithms and methods with their advantages/limitations\"\n",
    "\" - Practical implementations and computational considerations\"\n",
    "\" - Real-world applications and case studies\"\n",
    "\n",
    "\"The final report should be in Markdown format, with particular attention to the following:\"\n",
    "\" - Include clear mathematical formulations (use the LaTeX syntax when necessary)\\n\"\n",
    "\"- Describe the algorithms precisely, possibly with pseudo-code\\n\"\n",
    "\"- Suggest relevant visualizations (by describing them verbatim)\\n\"\n",
    "\"- Analyze performance and computational complexity\\n\"\n",
    "\"- Compare different methodological approaches\\n\\n\"\n",
    "\n",
    "\"The report should be detailed and thorough, aiming for 5-10 pages (minimum 1000 words). Use technical language appropriate for an audience of signal processing engineers and researchers.\\n\\n\"\n",
    "\n",
    "\"For follow-up questions, propose exactly 5 unique questions that could extend this research, focusing on:\\n\"\n",
    "\"- New trends or emerging techniques\\n\"\n",
    "\"- Potential algorithmic optimizations\\n\"\n",
    "\"- Interdisciplinary applications\\n\"\n",
    "\"- Unsolved technical challenges\\n\"\n",
    "\"- Promising future directions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our Writer Agent using the Agent class from the OpenAI Agents SDK. This agent will synthesize all the research findings into a comprehensive report. We're configuring it with the `ReportData` output type to structure the response with a short summary, markdown report, and follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportData(BaseModel):\n",
    "    short_summary: str\n",
    "    \"\"\"A short 2-3 sentence summary of the findings.\"\"\"\n",
    "\n",
    "    markdown_report: str\n",
    "    \"\"\"The final report\"\"\"\n",
    "\n",
    "    follow_up_questions: list[str]\n",
    "    \"\"\"Suggested topics to research further\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define our Writer Agent using the Agent class from the OpenAI Agents SDK. This agent will take the original query and research summaries, then synthesize them into a comprehensive report with follow-up questions. We've defined a custom output type called `ReportData` that structures the response with a short summary, markdown report, and follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_agent = Agent(\n",
    "    name=\"WriterAgent\",\n",
    "    instructions=WRITER_PROMPT,\n",
    "    model=\"o3-mini\",\n",
    "    output_type=ReportData,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Question #3: \n",
    "\n",
    "Why are we electing to use a reasoning model for writing our report?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 😎 ANSWER #3:\n",
    "\n",
    "This type of model excels in tasks requiring step-by-step reasoning. \n",
    "It also allows a more methodical organization of information, the breakdown of complex problems, and the structure of the report according to clear logic.\n",
    "Finally, this type of model allows for better integration of the various sources of information from the research, assessment of their relevance, and organization of them into a coherent report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Create Utility Classes \n",
    "\n",
    "We'll define utility classes to help with displaying progress and managing the research workflow. The Printer class below will provide real-time updates on the research process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Printer class provides real-time progress updates during the research process. It uses Rich's Live display to show dynamic content with spinners for in-progress items and checkmarks for completed tasks. The class maintains a dictionary of items with their completion status and can selectively hide checkmarks for specific items. This creates a clean, interactive console experience that keeps the user informed about the current state of the research workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from rich.console import Console, Group\n",
    "from rich.live import Live\n",
    "from rich.spinner import Spinner\n",
    "\n",
    "class Printer:\n",
    "    def __init__(self, console: Console):\n",
    "        self.live = Live(console=console)\n",
    "        self.items: dict[str, tuple[str, bool]] = {}\n",
    "        self.hide_done_ids: set[str] = set()\n",
    "        self.live.start()\n",
    "\n",
    "    def end(self) -> None:\n",
    "        self.live.stop()\n",
    "\n",
    "    def hide_done_checkmark(self, item_id: str) -> None:\n",
    "        self.hide_done_ids.add(item_id)\n",
    "\n",
    "    def update_item(\n",
    "        self, item_id: str, content: str, is_done: bool = False, hide_checkmark: bool = False\n",
    "    ) -> None:\n",
    "        self.items[item_id] = (content, is_done)\n",
    "        if hide_checkmark:\n",
    "            self.hide_done_ids.add(item_id)\n",
    "        self.flush()\n",
    "\n",
    "    def mark_item_done(self, item_id: str) -> None:\n",
    "        self.items[item_id] = (self.items[item_id][0], True)\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self) -> None:\n",
    "        renderables: list[Any] = []\n",
    "        for item_id, (content, is_done) in self.items.items():\n",
    "            if is_done:\n",
    "                prefix = \"✅ \" if item_id not in self.hide_done_ids else \"\"\n",
    "                renderables.append(prefix + content)\n",
    "            else:\n",
    "                renderables.append(Spinner(\"dots\", text=content))\n",
    "        self.live.update(Group(*renderables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a ResearchManager class that will orchestrate the research process. This class will:\n",
    "1. Plan searches based on the query\n",
    "2. Perform those searches to gather information\n",
    "3. Write a comprehensive report based on the gathered information\n",
    "4. Display progress using our Printer class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "from agents import Runner, custom_span, gen_trace_id, trace\n",
    "\n",
    "class ResearchManager:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        self.printer = Printer(self.console)\n",
    "\n",
    "    async def run(self, query: str) -> None:\n",
    "        trace_id = gen_trace_id()\n",
    "        with trace(\"Research trace\", trace_id=trace_id):\n",
    "            self.printer.update_item(\n",
    "                \"trace_id\",\n",
    "                f\"View trace: https://platform.openai.com/traces/trace?trace_id={trace_id}\",\n",
    "                is_done=True,\n",
    "                hide_checkmark=True,\n",
    "            )\n",
    "\n",
    "            self.printer.update_item(\n",
    "                \"starting\",\n",
    "                \"Starting research...\",\n",
    "                is_done=True,\n",
    "                hide_checkmark=True,\n",
    "            )\n",
    "            search_plan = await self._plan_searches(query)\n",
    "            search_results = await self._perform_searches(search_plan)\n",
    "            report = await self._write_report(query, search_results)\n",
    "\n",
    "            final_report = f\"Report summary\\n\\n{report.short_summary}\"\n",
    "            self.printer.update_item(\"final_report\", final_report, is_done=True)\n",
    "\n",
    "            self.printer.end()\n",
    "\n",
    "        print(\"\\n\\n=====REPORT=====\\n\\n\")\n",
    "        print(f\"Report: {report.markdown_report}\")\n",
    "        print(\"\\n\\n=====FOLLOW UP QUESTIONS=====\\n\\n\")\n",
    "        unique_questions = []\n",
    "        seen = set()\n",
    "        \n",
    "        for question in report.follow_up_questions:\n",
    "            if question not in seen:\n",
    "                unique_questions.append(question)\n",
    "                seen.add(question)\n",
    "        \n",
    "        for i, question in enumerate(unique_questions, 1):\n",
    "            print(f\"{i}. {question}\")\n",
    "\n",
    "    async def _plan_searches(self, query: str) -> WebSearchPlan:\n",
    "        self.printer.update_item(\"planning\", \"Planning searches...\")\n",
    "        result = await Runner.run(\n",
    "            planner_agent,\n",
    "            f\"Query: {query}\",\n",
    "        )\n",
    "        self.printer.update_item(\n",
    "            \"planning\",\n",
    "            f\"Will perform {len(result.final_output.searches)} searches\",\n",
    "            is_done=True,\n",
    "        )\n",
    "        return result.final_output_as(WebSearchPlan)\n",
    "\n",
    "    async def _perform_searches(self, search_plan: WebSearchPlan) -> list[str]:\n",
    "        with custom_span(\"Search the web\"):\n",
    "            self.printer.update_item(\"searching\", \"Searching...\")\n",
    "            num_completed = 0\n",
    "            max_concurrent = 5\n",
    "            results = []\n",
    "            \n",
    "            for i in range(0, len(search_plan.searches), max_concurrent):\n",
    "                batch = search_plan.searches[i:i+max_concurrent]\n",
    "                tasks = [asyncio.create_task(self._search(item)) for item in batch]\n",
    "                \n",
    "                for task in asyncio.as_completed(tasks):\n",
    "                    try:\n",
    "                        result = await task\n",
    "                        if result is not None:\n",
    "                            results.append(result)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Search error: {e}\")\n",
    "                        \n",
    "                    num_completed += 1\n",
    "                    self.printer.update_item(\n",
    "                        \"searching\", f\"Searching... {num_completed}/{len(search_plan.searches)} completed\"\n",
    "                    )\n",
    "            \n",
    "            self.printer.mark_item_done(\"searching\")\n",
    "            return results\n",
    "\n",
    "    async def _search(self, item: WebSearchItem) -> str | None:\n",
    "        input = f\"Search term: {item.query}\\nReason for searching: {item.reason}\"\n",
    "        try:\n",
    "            result = await Runner.run(\n",
    "                search_agent,\n",
    "                input,\n",
    "            )\n",
    "            return str(result.final_output)\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for '{item.query}': {e}\")\n",
    "            return None\n",
    "\n",
    "    async def _write_report(self, query: str, search_results: list[str]) -> ReportData:\n",
    "        self.printer.update_item(\"writing\", \"Thinking about report...\")\n",
    "        input = f\"Original query: {query}\\nSummarized search results: {search_results}\"\n",
    "        \n",
    "        result = Runner.run_streamed(\n",
    "            writer_agent,\n",
    "            input,\n",
    "        )\n",
    "        \n",
    "        update_messages = [\n",
    "            \"Thinking about report...\",\n",
    "            \"Planning report structure...\",\n",
    "            \"Writing outline...\",\n",
    "            \"Creating sections...\",\n",
    "            \"Cleaning up formatting...\",\n",
    "            \"Finalizing report...\",\n",
    "            \"Finishing report...\",\n",
    "        ]\n",
    "\n",
    "        last_update = time.time()\n",
    "        next_message = 0\n",
    "        \n",
    "        async for event in result.stream_events():\n",
    "            if time.time() - last_update > 5 and next_message < len(update_messages):\n",
    "                self.printer.update_item(\"writing\", update_messages[next_message])\n",
    "                next_message += 1\n",
    "                last_update = time.time()\n",
    "\n",
    "        self.printer.mark_item_done(\"writing\")\n",
    "        return result.final_output_as(ReportData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🏗️ Activity #2:\n",
    "\n",
    "Convert the above flow into a flowchart style image (software of your choosing, but if you're not sure which to use try [Excallidraw](https://excalidraw.com/)) that outlines how the different Agents interact with each other. \n",
    "\n",
    "> HINT: Cursor's CMD+L (CTRL+L on Windows) would be a helpful way to get a basic diagram that you can add more detail to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SDK_Diagram](./images/SDK_Diagram_AIE6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Running Our Agent\n",
    "\n",
    "Now let's run our agent! The main function below will prompt the user for a research topic, then pass that query to our ResearchManager to handle the entire research process. The ResearchManager will: \n",
    "\n",
    "1. Break down the query into search items\n",
    "2. Search for information on each item\n",
    "3. Write a comprehensive report based on the search results\n",
    "\n",
    "Let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main() -> None:\n",
    "    query = input(\"What would you like to research? \")\n",
    "    await ResearchManager().run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867bac2657724a0aa8b96a9856918358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=====REPORT=====\n",
      "\n",
      "\n",
      "Report: # Signal Processing in Speech-to-Text Pipelines: A Comprehensive Analysis\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Speech-to-text (STT) systems are at the forefront of human-computer interaction, enabling applications ranging from virtual assistants to automated transcription services. Central to these systems is the role of signal processing, which transforms raw audio signals into representations that can be effectively analyzed and transcribed by computational models. Signal processing in STT pipelines involves a sequence of operations including pre-processing, feature extraction, acoustic modeling, language modeling, and decoding. Each step plays a critical role in handling challenges such as noise reduction, time-frequency analysis, and the extraction of perceptually relevant information.\n",
      "\n",
      "This report provides a detailed exploration of the mathematical and technical foundations underlying signal processing in STT, describes key algorithms and methods, discusses practical implementation strategies, and examines computational considerations. We also present a number of case studies on real-world applications to highlight the challenges and successes in modern speech recognition systems.\n",
      "\n",
      "## Table of Contents\n",
      "\n",
      "1. [Mathematical and Technical Foundations](#mathematical-and-technical-foundations)\n",
      "2. [Algorithms and Methods](#algorithms-and-methods)\n",
      "   - [Time-Frequency Analysis](#time-frequency-analysis)\n",
      "   - [Feature Extraction Techniques](#feature-extraction-techniques)\n",
      "   - [Noise Reduction and Filtering](#noise-reduction-and-filtering)\n",
      "3. [Practical Implementations and Computational Considerations](#practical-implementations-and-computational-considerations)\n",
      "   - [Algorithmic Complexity](#algorithmic-complexity)\n",
      "   - [Systems Integration and Hardware Constraints](#systems-integration-and-hardware-constraints)\n",
      "4. [Real-World Applications and Case Studies](#real-world-applications-and-case-studies)\n",
      "5. [Conclusion](#conclusion)\n",
      "\n",
      "## Mathematical and Technical Foundations\n",
      "\n",
      "The foundation of signal processing in speech analysis is built upon core mathematical tools that enable the conversion of time-domain signals into representations that reveal underlying spectral properties. Key among these tools is the Fourier Transform, which decomposes a signal into its sine and cosine constituents:\n",
      "\n",
      "\\[\n",
      "X(f) = \\int_{-\\infty}^{\\infty} x(t) e^{-j2\\pi ft} dt\n",
      "\\]\n",
      "\n",
      "In practice, the Discrete Fourier Transform (DFT) and its efficient implementation via the Fast Fourier Transform (FFT) are used to analyze digital signals. The concept of windowing, where signals are segmented into finite-length frames, leads to Short-Time Fourier Transforms (STFT) that provide time-frequency representation:\n",
      "\n",
      "\\[\n",
      "\\text{STFT}\\{x(t)\\}(\\tau,f) = \\int_{-\\infty}^{\\infty} x(t) w(t-\\tau) e^{-j2\\pi ft} dt\n",
      "\\]\n",
      "\n",
      "This mathematical framework not only underpins spectral analysis but also facilitates the extraction of features that mimic human auditory perception, such as Mel-Frequency Cepstral Coefficients (MFCCs). Using a mel scale transformation, the spectrum is warped to better reflect the non-linear response of the human ear, which is typically described through triangular filter banks and logarithmic compression:\n",
      "\n",
      "\\[\n",
      "\\text{MFCC}(n) = \\sum_{k=1}^{K} \\log \\left( |X(k)| \\right) \\cos \\left[ \\frac{\\pi n}{K} (k - 0.5) \\right]\n",
      "\\]\n",
      "\n",
      "In addition to the Fourier analysis, techniques like Linear Predictive Coding (LPC) model the speech signal based on the principle that current samples can be predicted as a linear combination of past samples. Such mathematical formulations provide a compact representation of the vocal tract’s resonance behavior.\n",
      "\n",
      "## Algorithms and Methods\n",
      "\n",
      "The development of STT systems leverages a variety of algorithms that fall into several categories. Below we describe some of the fundamental and modern methods used in the pipelines:\n",
      "\n",
      "### Time-Frequency Analysis\n",
      "\n",
      "- **Fourier Transform and STFT**: These techniques are employed to break a continuous-time speech signal into frequency components over time. The primary benefit is the clear visualization of spectral transitions, which is essential for the identification of phonemes. A recommended visualization is the spectrogram, where the horizontal axis represents time, the vertical axis frequency, and the intensity indicates the amplitude of each component.\n",
      "\n",
      "### Feature Extraction Techniques\n",
      "\n",
      "- **Mel-Frequency Cepstral Coefficients (MFCCs)**: MFCCs are widely used due to their ability to represent the perceptually relevant properties of speech. The process involves pre-emphasis, framing, windowing, FFT, mel filterbank processing, logarithmic non-linearity, and ultimately, a Discrete Cosine Transform (DCT) to decorrelate the features. The pseudo-code below summarizes the process:\n",
      "\n",
      "  ```plaintext\n",
      "  function computeMFCC(signal, sample_rate):\n",
      "      pre_emphasized = pre_emphasize(signal)\n",
      "      frames = frame_signal(pre_emphasized, frame_size, overlap)\n",
      "      windowed_frames = apply_window(frames, window_function)\n",
      "      for each frame in windowed_frames:\n",
      "          spectrum = FFT(frame)\n",
      "          mel_spectrum = apply_mel_filterbank(spectrum, sample_rate)\n",
      "          log_mel = log(mel_spectrum)\n",
      "          mfccs = DCT(log_mel)\n",
      "          store(mfccs)\n",
      "      return collected_mfccs\n",
      "  ```\n",
      "\n",
      "- **Linear Predictive Coding (LPC)**: This method uses a prediction model to approximate the spectral envelope of the speech signal. It is particularly useful in speech coding and synthesis.\n",
      "\n",
      "- **Cepstral Analysis and Wavelet Transforms**: Cepstral analysis further decouples the contributions of the vocal tract and the excitation signal, while wavelet transforms provide multi-resolution analysis capturing both time and frequency details. Such methods are beneficial when analyzing transient phenomena in speech.\n",
      "\n",
      "### Noise Reduction and Filtering\n",
      "\n",
      "Noise in STT pipelines can degrade accuracy. Several methods exist:\n",
      "\n",
      "- **Spectral Subtraction and Wiener Filtering**: These techniques estimate the noise spectrum and subtract or filter it from the speech signal, enhancing clarity and intelligibility.\n",
      "\n",
      "- **RASTA Filtering**: RASTA (RelAtive SpecTrAl) filtering is applied in the logarithmic spectral domain to mitigate slow varying channel effects.\n",
      "\n",
      "- **Adaptive Filtering**: Techniques that continuously adjust filter parameters based on current noise statistics are invaluable in dynamic environments.\n",
      "\n",
      "## Practical Implementations and Computational Considerations\n",
      "\n",
      "Implementing signal processing algorithms in STT pipelines requires careful consideration of computational efficiency and system constraints. Below we outline some important factors:\n",
      "\n",
      "### Algorithmic Complexity\n",
      "\n",
      "- **FFT Complexity**: FFT algorithms operate in \\(O(N \\log N)\\) where \\(N\\) is the number of points. This efficiency is vital for processing real-time audio streams.\n",
      "\n",
      "- **DCT and LPC**: The DCT used in computing MFCCs and the matrix inversions required in LPC estimation are computationally intensive but are typically optimized using linear algebra libraries.\n",
      "\n",
      "### Systems Integration and Hardware Constraints\n",
      "\n",
      "- **Real-Time Processing**: For STT systems employed in applications such as virtual assistants or live captioning, the algorithms must operate with minimal latency. Efficient memory management and parallel processing techniques are often leveraged.\n",
      "\n",
      "- **Embedded Systems**: Devices like smartphones or IoT sensors necessitate lightweight models. Here, deep learning-based approaches might be combined with traditional signal processing to strike a balance between accuracy and computational resource usage.\n",
      "\n",
      "- **Software Environments**: Implementation is commonly done in environments like MATLAB, Python (with NumPy, SciPy, and PyTorch), or specialized DSP hardware. In such cases, algorithm optimizations and vectorized computations can significantly reduce runtime.\n",
      "\n",
      "### Pseudo-Code for a Real-Time Preprocessing Pipeline\n",
      "\n",
      "Below is an example of a simplified pseudo-code for real-time audio pre-processing:\n",
      "\n",
      "```plaintext\n",
      "initialize_audio_stream(sample_rate, buffer_size)\n",
      "while audio_stream is active:\n",
      "    raw_signal = read_audio_data(buffer_size)\n",
      "    denoised_signal = wiener_filter(raw_signal)\n",
      "    pre_emphasized_signal = pre_emphasize(denoised_signal)\n",
      "    frames = frame_signal(pre_emphasized_signal)\n",
      "    for each frame in frames:\n",
      "         features = extract_features(frame)  // e.g., MFCCs\n",
      "         pass features to acoustic model\n",
      "```\n",
      "\n",
      "This pipeline illustrates how signal processing seamlessly integrates with machine learning components in a speech recognition system.\n",
      "\n",
      "## Real-World Applications and Case Studies\n",
      "\n",
      "Several case studies demonstrate the practical efficacy of signal processing techniques in STT systems:\n",
      "\n",
      "- **Voice Assistants**: Modern assistants (e.g., Siri, Google Assistant) rely on robust signal pre-processing to discern commands even against noisy backgrounds. Techniques like noise reduction and dynamic range compression ensure that the speech signal is clear before feature extraction.\n",
      "\n",
      "- **Medical Transcription**: In environments with complex acoustic profiles, speech-to-text systems utilize advanced noise filtering algorithms and adaptive filtering to improve transcription of clinical data.\n",
      "\n",
      "- **Multilingual STT Systems**: Enhanced feature extraction methods combined with language modeling are crucial for multilingual applications. For instance, transformer-based architectures and enhanced spectral methods, such as those incorporating wavelet transforms, have improved the performance of systems like OpenAI's Whisper and Speech-LLaMA.\n",
      "\n",
      "- **Real-Time Communication**: In applications such as live captioning for broadcasts or teleconferencing, efficient endpoint detection and frame segmentation, which rely on both STFT and energy-based methods, are critical for maintaining low latency and high accuracy.\n",
      "\n",
      "- **Embedded Devices**: Ultra-lightweight networks, e.g., FSPEN, utilize dual-stream processing (full-band and subband) to operate under the constraints of TWS headsets and IoT devices. These implementations balance power consumption and computational load while providing real-time enhancement, even for compressed signals.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Signal processing is the cornerstone of modern speech-to-text pipelines. From the mathematical principles governing time-frequency analysis to the practical challenges of noise reduction and real-time processing, the integration of these techniques has profoundly shaped the evolution of STT systems. Traditional methods like MFCCs and LPC remain relevant, while novel approaches—especially those rooted in deep learning and transformer-based architectures—are pushing the boundaries of accuracy and robustness.\n",
      "\n",
      "The journey from raw audio input to accurate textual transcription involves a series of complex, interdependent processes. As computational resources continue to expand and algorithms become more sophisticated, the future will likely see even tighter integration between conventional DSP methods and emerging techniques such as differentiable digital signal processing (DDSP) and multi-modal learning approaches. Continued research and cross-disciplinary innovation will be essential in addressing the persistent challenges and exploring new applications in an increasingly connected digital world.\n",
      "\n",
      "---\n",
      "\n",
      "*Visualizations Recommended*: \n",
      "\n",
      "- Spectrogram images to illustrate the time-frequency distribution of speech signals\n",
      "- Flowcharts or block diagrams of STT pipelines, highlighting preprocessing, feature extraction, and decoding stages\n",
      "- Complexity graphs comparing traditional methods (e.g., FFT, DCT, LPC) with deep learning models\n",
      "\n",
      "*Performance Analysis*: \n",
      "\n",
      "Various studies have shown that improved signal processing methods lead to higher SNR values and hence better transcription accuracy. A comparative analysis of word recognition rates as a function of noise levels, computational load, and latency provides insights into optimal trade-offs between accuracy and resource expenditure.\n",
      "\n",
      "*Comparative Insights*: \n",
      "\n",
      "While traditional methods excel in computational efficiency and explainability, modern deep learning approaches (e.g., CNNs, RNNs, Transformers) offer superior adaptability in dynamic environments but at the cost of increased computational complexity. Hybrid approaches promise to combine the best of both worlds, ensuring robustness even under stringent resource constraints.\n",
      "\n",
      "\n",
      "\n",
      "=====FOLLOW UP QUESTIONS=====\n",
      "\n",
      "\n",
      "1. What emerging deep learning architectures could further enhance the efficiency of STT pipelines, particularly under noisy conditions?\n",
      "2. How can algorithmic optimizations balance the trade-off between computational complexity and real-time performance in embedded STT applications?\n",
      "3. In what ways can interdisciplinary methods (e.g., from neuroscience or linguistics) inform the design of new feature extraction techniques for speech recognition?\n",
      "4. What are the main unsolved challenges in accurately modeling the temporal dynamics of speech signals under extreme environmental noise?\n",
      "5. How might the integration of differentiable digital signal processing (DDSP) contribute to the development of end-to-end optimized STT systems in a multi-modal framework?\n"
     ]
    }
   ],
   "source": [
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Processing in Speech-to-Text Pipelines: A Comprehensive Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Speech-to-text (STT) systems are at the forefront of human-computer interaction, enabling applications ranging from virtual assistants to automated transcription services. Central to these systems is the role of signal processing, which transforms raw audio signals into representations that can be effectively analyzed and transcribed by computational models. Signal processing in STT pipelines involves a sequence of operations including pre-processing, feature extraction, acoustic modeling, language modeling, and decoding. Each step plays a critical role in handling challenges such as noise reduction, time-frequency analysis, and the extraction of perceptually relevant information.\n",
    "\n",
    "This report provides a detailed exploration of the mathematical and technical foundations underlying signal processing in STT, describes key algorithms and methods, discusses practical implementation strategies, and examines computational considerations. We also present a number of case studies on real-world applications to highlight the challenges and successes in modern speech recognition systems.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Mathematical and Technical Foundations](#mathematical-and-technical-foundations)\n",
    "2. [Algorithms and Methods](#algorithms-and-methods)\n",
    "   - [Time-Frequency Analysis](#time-frequency-analysis)\n",
    "   - [Feature Extraction Techniques](#feature-extraction-techniques)\n",
    "   - [Noise Reduction and Filtering](#noise-reduction-and-filtering)\n",
    "3. [Practical Implementations and Computational Considerations](#practical-implementations-and-computational-considerations)\n",
    "   - [Algorithmic Complexity](#algorithmic-complexity)\n",
    "   - [Systems Integration and Hardware Constraints](#systems-integration-and-hardware-constraints)\n",
    "4. [Real-World Applications and Case Studies](#real-world-applications-and-case-studies)\n",
    "5. [Conclusion](#conclusion)\n",
    "\n",
    "## Mathematical and Technical Foundations\n",
    "\n",
    "The foundation of signal processing in speech analysis is built upon core mathematical tools that enable the conversion of time-domain signals into representations that reveal underlying spectral properties. Key among these tools is the Fourier Transform, which decomposes a signal into its sine and cosine constituents:\n",
    "\n",
    "\\[\n",
    "X(f) = \\int_{-\\infty}^{\\infty} x(t) e^{-j2\\pi ft} dt\n",
    "\\]\n",
    "\n",
    "In practice, the Discrete Fourier Transform (DFT) and its efficient implementation via the Fast Fourier Transform (FFT) are used to analyze digital signals. The concept of windowing, where signals are segmented into finite-length frames, leads to Short-Time Fourier Transforms (STFT) that provide time-frequency representation:\n",
    "\n",
    "\\[\n",
    "\\text{STFT}\\{x(t)\\}(\\tau,f) = \\int_{-\\infty}^{\\infty} x(t) w(t-\\tau) e^{-j2\\pi ft} dt\n",
    "\\]\n",
    "\n",
    "This mathematical framework not only underpins spectral analysis but also facilitates the extraction of features that mimic human auditory perception, such as Mel-Frequency Cepstral Coefficients (MFCCs). Using a mel scale transformation, the spectrum is warped to better reflect the non-linear response of the human ear, which is typically described through triangular filter banks and logarithmic compression:\n",
    "\n",
    "\\[\n",
    "\\text{MFCC}(n) = \\sum_{k=1}^{K} \\log \\left( |X(k)| \\right) \\cos \\left[ \\frac{\\pi n}{K} (k - 0.5) \\right]\n",
    "\\]\n",
    "\n",
    "In addition to the Fourier analysis, techniques like Linear Predictive Coding (LPC) model the speech signal based on the principle that current samples can be predicted as a linear combination of past samples. Such mathematical formulations provide a compact representation of the vocal tract’s resonance behavior.\n",
    "\n",
    "## Algorithms and Methods\n",
    "\n",
    "The development of STT systems leverages a variety of algorithms that fall into several categories. Below we describe some of the fundamental and modern methods used in the pipelines:\n",
    "\n",
    "### Time-Frequency Analysis\n",
    "\n",
    "- **Fourier Transform and STFT**: These techniques are employed to break a continuous-time speech signal into frequency components over time. The primary benefit is the clear visualization of spectral transitions, which is essential for the identification of phonemes. A recommended visualization is the spectrogram, where the horizontal axis represents time, the vertical axis frequency, and the intensity indicates the amplitude of each component.\n",
    "\n",
    "### Feature Extraction Techniques\n",
    "\n",
    "- **Mel-Frequency Cepstral Coefficients (MFCCs)**: MFCCs are widely used due to their ability to represent the perceptually relevant properties of speech. The process involves pre-emphasis, framing, windowing, FFT, mel filterbank processing, logarithmic non-linearity, and ultimately, a Discrete Cosine Transform (DCT) to decorrelate the features. The pseudo-code below summarizes the process:\n",
    "\n",
    "  ```plaintext\n",
    "  function computeMFCC(signal, sample_rate):\n",
    "      pre_emphasized = pre_emphasize(signal)\n",
    "      frames = frame_signal(pre_emphasized, frame_size, overlap)\n",
    "      windowed_frames = apply_window(frames, window_function)\n",
    "      for each frame in windowed_frames:\n",
    "          spectrum = FFT(frame)\n",
    "          mel_spectrum = apply_mel_filterbank(spectrum, sample_rate)\n",
    "          log_mel = log(mel_spectrum)\n",
    "          mfccs = DCT(log_mel)\n",
    "          store(mfccs)\n",
    "      return collected_mfccs\n",
    "  ```\n",
    "\n",
    "- **Linear Predictive Coding (LPC)**: This method uses a prediction model to approximate the spectral envelope of the speech signal. It is particularly useful in speech coding and synthesis.\n",
    "\n",
    "- **Cepstral Analysis and Wavelet Transforms**: Cepstral analysis further decouples the contributions of the vocal tract and the excitation signal, while wavelet transforms provide multi-resolution analysis capturing both time and frequency details. Such methods are beneficial when analyzing transient phenomena in speech.\n",
    "\n",
    "### Noise Reduction and Filtering\n",
    "\n",
    "Noise in STT pipelines can degrade accuracy. Several methods exist:\n",
    "\n",
    "- **Spectral Subtraction and Wiener Filtering**: These techniques estimate the noise spectrum and subtract or filter it from the speech signal, enhancing clarity and intelligibility.\n",
    "\n",
    "- **RASTA Filtering**: RASTA (RelAtive SpecTrAl) filtering is applied in the logarithmic spectral domain to mitigate slow varying channel effects.\n",
    "\n",
    "- **Adaptive Filtering**: Techniques that continuously adjust filter parameters based on current noise statistics are invaluable in dynamic environments.\n",
    "\n",
    "## Practical Implementations and Computational Considerations\n",
    "\n",
    "Implementing signal processing algorithms in STT pipelines requires careful consideration of computational efficiency and system constraints. Below we outline some important factors:\n",
    "\n",
    "### Algorithmic Complexity\n",
    "\n",
    "- **FFT Complexity**: FFT algorithms operate in \\(O(N \\log N)\\) where \\(N\\) is the number of points. This efficiency is vital for processing real-time audio streams.\n",
    "\n",
    "- **DCT and LPC**: The DCT used in computing MFCCs and the matrix inversions required in LPC estimation are computationally intensive but are typically optimized using linear algebra libraries.\n",
    "\n",
    "### Systems Integration and Hardware Constraints\n",
    "\n",
    "- **Real-Time Processing**: For STT systems employed in applications such as virtual assistants or live captioning, the algorithms must operate with minimal latency. Efficient memory management and parallel processing techniques are often leveraged.\n",
    "\n",
    "- **Embedded Systems**: Devices like smartphones or IoT sensors necessitate lightweight models. Here, deep learning-based approaches might be combined with traditional signal processing to strike a balance between accuracy and computational resource usage.\n",
    "\n",
    "- **Software Environments**: Implementation is commonly done in environments like MATLAB, Python (with NumPy, SciPy, and PyTorch), or specialized DSP hardware. In such cases, algorithm optimizations and vectorized computations can significantly reduce runtime.\n",
    "\n",
    "### Pseudo-Code for a Real-Time Preprocessing Pipeline\n",
    "\n",
    "Below is an example of a simplified pseudo-code for real-time audio pre-processing:\n",
    "\n",
    "```plaintext\n",
    "initialize_audio_stream(sample_rate, buffer_size)\n",
    "while audio_stream is active:\n",
    "    raw_signal = read_audio_data(buffer_size)\n",
    "    denoised_signal = wiener_filter(raw_signal)\n",
    "    pre_emphasized_signal = pre_emphasize(denoised_signal)\n",
    "    frames = frame_signal(pre_emphasized_signal)\n",
    "    for each frame in frames:\n",
    "         features = extract_features(frame)  // e.g., MFCCs\n",
    "         pass features to acoustic model\n",
    "```\n",
    "\n",
    "This pipeline illustrates how signal processing seamlessly integrates with machine learning components in a speech recognition system.\n",
    "\n",
    "## Real-World Applications and Case Studies\n",
    "\n",
    "Several case studies demonstrate the practical efficacy of signal processing techniques in STT systems:\n",
    "\n",
    "- **Voice Assistants**: Modern assistants (e.g., Siri, Google Assistant) rely on robust signal pre-processing to discern commands even against noisy backgrounds. Techniques like noise reduction and dynamic range compression ensure that the speech signal is clear before feature extraction.\n",
    "\n",
    "- **Medical Transcription**: In environments with complex acoustic profiles, speech-to-text systems utilize advanced noise filtering algorithms and adaptive filtering to improve transcription of clinical data.\n",
    "\n",
    "- **Multilingual STT Systems**: Enhanced feature extraction methods combined with language modeling are crucial for multilingual applications. For instance, transformer-based architectures and enhanced spectral methods, such as those incorporating wavelet transforms, have improved the performance of systems like OpenAI's Whisper and Speech-LLaMA.\n",
    "\n",
    "- **Real-Time Communication**: In applications such as live captioning for broadcasts or teleconferencing, efficient endpoint detection and frame segmentation, which rely on both STFT and energy-based methods, are critical for maintaining low latency and high accuracy.\n",
    "\n",
    "- **Embedded Devices**: Ultra-lightweight networks, e.g., FSPEN, utilize dual-stream processing (full-band and subband) to operate under the constraints of TWS headsets and IoT devices. These implementations balance power consumption and computational load while providing real-time enhancement, even for compressed signals.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Signal processing is the cornerstone of modern speech-to-text pipelines. From the mathematical principles governing time-frequency analysis to the practical challenges of noise reduction and real-time processing, the integration of these techniques has profoundly shaped the evolution of STT systems. Traditional methods like MFCCs and LPC remain relevant, while novel approaches—especially those rooted in deep learning and transformer-based architectures—are pushing the boundaries of accuracy and robustness.\n",
    "\n",
    "The journey from raw audio input to accurate textual transcription involves a series of complex, interdependent processes. As computational resources continue to expand and algorithms become more sophisticated, the future will likely see even tighter integration between conventional DSP methods and emerging techniques such as differentiable digital signal processing (DDSP) and multi-modal learning approaches. Continued research and cross-disciplinary innovation will be essential in addressing the persistent challenges and exploring new applications in an increasingly connected digital world.\n",
    "\n",
    "---\n",
    "\n",
    "*Visualizations Recommended*: \n",
    "\n",
    "- Spectrogram images to illustrate the time-frequency distribution of speech signals\n",
    "- Flowcharts or block diagrams of STT pipelines, highlighting preprocessing, feature extraction, and decoding stages\n",
    "- Complexity graphs comparing traditional methods (e.g., FFT, DCT, LPC) with deep learning models\n",
    "\n",
    "*Performance Analysis*: \n",
    "\n",
    "Various studies have shown that improved signal processing methods lead to higher SNR values and hence better transcription accuracy. A comparative analysis of word recognition rates as a function of noise levels, computational load, and latency provides insights into optimal trade-offs between accuracy and resource expenditure.\n",
    "\n",
    "*Comparative Insights*: \n",
    "\n",
    "While traditional methods excel in computational efficiency and explainability, modern deep learning approaches (e.g., CNNs, RNNs, Transformers) offer superior adaptability in dynamic environments but at the cost of increased computational complexity. Hybrid approaches promise to combine the best of both worlds, ensuring robustness even under stringent resource constraints.\n",
    "\n",
    "\n",
    "\n",
    "=====FOLLOW UP QUESTIONS=====\n",
    "\n",
    "\n",
    "1. What emerging deep learning architectures could further enhance the efficiency of STT pipelines, particularly under noisy conditions?\n",
    "2. How can algorithmic optimizations balance the trade-off between computational complexity and real-time performance in embedded STT applications?\n",
    "3. In what ways can interdisciplinary methods (e.g., from neuroscience or linguistics) inform the design of new feature extraction techniques for speech recognition?\n",
    "4. What are the main unsolved challenges in accurately modeling the temporal dynamics of speech signals under extreme environmental noise?\n",
    "5. How might the integration of differentiable digital signal processing (DDSP) contribute to the development of end-to-end optimized STT systems in a multi-modal framework?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sample Report in Markdown \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Agents SDK: A Comprehensive Report\n",
    "\n",
    "*Published: October 2023*\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Core Concepts and Key Features](#core-concepts-and-key-features)\n",
    "3. [Architecture and Developer Experience](#architecture-and-developer-experience)\n",
    "4. [Comparative Analysis with Alternative Frameworks](#comparative-analysis-with-alternative-frameworks)\n",
    "5. [Integrations and Real-World Applications](#integrations-and-real-world-applications)\n",
    "6. [Troubleshooting, Observability, and Debugging](#troubleshooting-observability-and-debugging)\n",
    "7. [Community Impact and Future Directions](#community-impact-and-future-directions)\n",
    "8. [Conclusion](#conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In March 2025, OpenAI released the Agents SDK, a groundbreaking, open-source framework aimed at simplifying the development of autonomous AI agents capable of performing intricate tasks with minimal human intervention. Designed with a Python-first approach, the SDK offers a minimal set of abstractions, yet provides all the necessary components to build, debug, and optimize multi-agent workflows. The release marked a significant milestone for developers who seek to integrate large language models (LLMs) with advanced task delegation mechanisms, enabling next-generation automation in various industries.\n",
    "\n",
    "The primary goal of the OpenAI Agents SDK is to streamline the creation of agentic applications by offering core primitives such as *agents*, *handoffs*, and *guardrails*. These primitives are essential for orchestrating autonomous AI systems that perform key functions such as web search, file operations, and even actions on a computer. This report delves into the SDK's features, its operational architecture, integration capabilities, and how it compares to other frameworks in the rapidly evolving landscape of AI development tools.\n",
    "\n",
    "## Core Concepts and Key Features\n",
    "\n",
    "### Agents\n",
    "\n",
    "At the heart of the SDK are **agents**—intelligent entities that encapsulate a specific set of instructions and tools. Each agent is built on top of a large language model and can be customized with its own personality, domain expertise, and operational directives. For example, a \"Math Tutor\" agent could be designed to solve math problems by explaining each step clearly.\n",
    "\n",
    "**Key elements of an agent include:**\n",
    "\n",
    "- **Instructions:** Specific guidelines that shape the agent's responses and behavior in the context of its designated role.\n",
    "- **Tools:** Predefined or dynamically integrated tools that the agent can leverage to access external resources (e.g., web search or file search functionalities).\n",
    "\n",
    "### Handoffs\n",
    "\n",
    "A unique feature introduced by the SDK is the concept of **handoffs**. Handoffs allow agents to delegate tasks to one another based on expertise and contextual needs. This orchestration paves the way for sophisticated workflows where multiple agents work in tandem, each contributing its specialized capabilities to complete a complex task.\n",
    "\n",
    "### Guardrails\n",
    "\n",
    "Safety and reliability remain a cornerstone in AI development, and the SDK introduces **guardrails** as a means of controlling input and output validation. Guardrails help ensure that agents operate within defined safety parameters, preventing unintended actions and mitigating risks associated with autonomous decision-making.\n",
    "\n",
    "### Built-in Debugging and Observability\n",
    "\n",
    "The development process is further enhanced by built-in **tracing and visualization tools**. These tools offer real-time insights into agent interactions, tool invocations, and decision-making pathways, thereby making debugging and optimization more accessible and systematic. The tracing functionality is a vital feature for developers looking to fine-tune agent performance in production environments.\n",
    "\n",
    "## Architecture and Developer Experience\n",
    "\n",
    "### Python-First Approach\n",
    "\n",
    "The SDK is inherently Python-based, making it highly accessible to the vast community of Python developers. By leveraging existing language features without introducing excessive abstractions, the SDK provides both simplicity and power. The installation is straightforward:\n",
    "\n",
    "```bash\n",
    "mkdir my_project\n",
    "cd my_project\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install openai-agents\n",
    "```\n",
    "\n",
    "Once installed, developers can create and configure agents with minimal boilerplate code. The emphasis on a minimal learning curve has been a significant point of praise among early adopters.\n",
    "\n",
    "### Developer Tools and Tutorials\n",
    "\n",
    "In addition to comprehensive official documentation available on OpenAI’s GitHub pages, the community has contributed numerous tutorials and code examples. Video tutorials by experts such as Sam Witteveen and James Briggs provide hands-on demonstrations, ranging from simple agent creation to more sophisticated scenarios involving parallel execution and advanced tool integrations.\n",
    "\n",
    "### Use of Python's Ecosystem\n",
    "\n",
    "The integration with Python’s ecosystem means that developers can immediately apply a range of established libraries and frameworks. For instance, utilizing Pydantic for input validation in guardrails or leveraging visualization libraries to display agent workflows are examples of how the SDK embraces the strengths of Python.\n",
    "\n",
    "## Comparative Analysis with Alternative Frameworks\n",
    "\n",
    "While the OpenAI Agents SDK has received acclaim for its simplicity and robust integration with OpenAI’s ecosystem, other frameworks such as LangGraph, CrewAI, and AutoGen have emerged as viable alternatives. Here’s how they compare:\n",
    "\n",
    "- **LangGraph:** Known for its graph-based architecture, LangGraph is ideal for handling complex and cyclical workflows that require sophisticated state management. However, it comes with a steeper learning curve, making it less accessible for projects that require quick prototyping.\n",
    "\n",
    "- **CrewAI:** Emphasizing a role-based multi-agent system, CrewAI excels in scenarios where collaboration among agents is critical. Its design promotes clear segregation of duties among different agents, which can be beneficial in customer service or large-scale business automation.\n",
    "\n",
    "- **AutoGen:** This framework supports flexible conversation patterns and diverse agent interactions, particularly useful in applications where adaptive dialogue is essential. Nevertheless, AutoGen may introduce additional overhead when managing state and coordinating multiple agents.\n",
    "\n",
    "In contrast, the OpenAI Agents SDK strikes an effective balance by offering a lightweight yet powerful toolset geared towards production readiness. Its strengths lie in its minimal abstractions, ease of integration with various tools (like web search and file search), and built-in observability features that are crucial for debugging and tracing agent interactions.\n",
    "\n",
    "## Integrations and Real-World Applications\n",
    "\n",
    "### Diverse Integrations\n",
    "\n",
    "The real power of the OpenAI Agents SDK surfaces when it is integrated with other systems and platforms. Notable integrations include:\n",
    "\n",
    "- **Box Integration:** Enhancing enterprise content management, Box has adopted the SDK to enable secure AI-powered data processing. This integration allows agents to reliably access and interpret proprietary data.\n",
    "\n",
    "- **Coinbase AgentKit:** With financial capabilities in mind, Coinbase introduced AgentKit, leveraging the SDK to incorporate financial operations and risk analysis directly into AI agents.\n",
    "\n",
    "- **Milvus and Ollama:** These integrations allow the SDK to handle high-performance data queries and run agents on local infrastructure respectively, ensuring both speed and privacy.\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "The versatility of the SDK lends itself to a multitude of applications:\n",
    "\n",
    "- **Customer Support:** Automated agents can be built to handle customer inquiries, providing faster and more accurate responses while reducing workload on human agents.\n",
    "\n",
    "- **Content Generation:** In marketing and media, agents can generate high-quality articles, detailed reports, and even code reviews with built-in content guidelines.\n",
    "\n",
    "- **Financial Analysis:** Specialized agents capable of real-time data fetching and market analysis can generate actionable insights for investors and analysts.\n",
    "\n",
    "- **Health and Wellness:** Custom agents can handle tasks such as appointment scheduling, patient record management, and even provide personalized fitness and dietary recommendations.\n",
    "\n",
    "- **Educational Tools:** Intelligent tutoring agents can assist students by providing personalized learning experiences and instant feedback on assignments.\n",
    "\n",
    "These applications underscore the SDK’s transformative potential across various industries, driving the trend towards increased automation and efficiency.\n",
    "\n",
    "## Troubleshooting, Observability, and Debugging\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "As with any cutting-edge technology, developers working with the OpenAI Agents SDK have encountered challenges:\n",
    "\n",
    "- **API Key Management:** Authentication errors due to missing or invalid API keys are common. The solution involves ensuring that the `OPENAI_API_KEY` environment variable is correctly set or programmatically configured using OpenAI’s helper functions.\n",
    "\n",
    "- **Rate Limitations:** Rate limits, an intrinsic challenge with API-based services, require developers to monitor dashboard usage and implement retry strategies with exponential backoff.\n",
    "\n",
    "- **Response Delays:** Network latency and high server loads can result in unexpected delays. Developers are advised to check network settings, adhere to best practices in setting request timeouts, and monitor OpenAI’s service status.\n",
    "\n",
    "### Built-In Tracing Capabilities\n",
    "\n",
    "The SDK provides robust tracing tools that log agent inputs, outputs, tool interactions, and error messages. This level of observability is crucial for debugging complex workflows and allows developers to visualize the agent’s decision-making process in real time. By configuring a `TracingConfig` object, developers can capture detailed insights and identify performance bottlenecks.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Prompt Engineering:** Refine prompts to reduce ambiguity and minimize unexpected outputs.\n",
    "- **Layered Validation:** Use guardrails extensively to ensure inputs and outputs are verified at multiple layers.\n",
    "- **Modular Design:** Break complex tasks into smaller, more manageable components using handoffs to delegate tasks appropriately.\n",
    "\n",
    "## Community Impact and Future Directions\n",
    "\n",
    "### Developer and Enterprise Adoption\n",
    "\n",
    "The release of the OpenAI Agents SDK has been met with enthusiasm within the developer community. Its ease of use, combined with comprehensive documentation and community-driven resources (such as tutorials on Class Central and DataCamp), has accelerated its adoption across educational, enterprise, and research sectors.\n",
    "\n",
    "Several leading organizations, including Box and Coinbase, have integrated the SDK into their workflows, demonstrating its capability to drive real-world business solutions. The open-source nature of the SDK, licensed under the MIT License, further encourages widespread industrial collaboration and innovation.\n",
    "\n",
    "### Future Prospects\n",
    "\n",
    "Looking forward, OpenAI plans to extend the SDK’s support beyond Python, potentially embracing other programming languages like JavaScript. Additionally, future updates are anticipated to expand tool integrations, further enhance safety mechanisms, and streamline the development of multi-agent ecosystems. Planned deprecations of older APIs, such as the Assistants API in favor of the more unified Responses API, underline the SDK’s evolving roadmap aimed at future-proofing agentic applications.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The OpenAI Agents SDK represents a significant step forward in the field of AI development. Its lightweight, Python-first framework facilitates the creation of autonomous agents that can handle a wide array of tasks—from simple inquiries to complex multi-agent systems. The SDK’s robust integration capabilities, combined with its focus on safety and observability, make it an ideal choice for both developers and enterprises seeking to build reliable, scalable agentic applications.\n",
    "\n",
    "In summary, the SDK not only lowers the barrier to entry for developing sophisticated AI applications but also sets the stage for further innovations as the ecosystem evolves. It is poised to become a standard toolkit for the next generation of AI-driven technologies, empowering users across sectors to achieve greater efficiency and creativity in task automation.\n",
    "\n",
    "---\n",
    "\n",
    "*For further reading, developers are encouraged to visit the official OpenAI documentation, join the community forums, and explore real-world use cases to deepen their understanding of this transformative tool.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
