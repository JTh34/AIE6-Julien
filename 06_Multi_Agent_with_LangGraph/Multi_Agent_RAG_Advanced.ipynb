{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jthomazo/Archives/01_Projets/02_AIM/AIE6/06_Multi_Agent_with_LangGraph/.venv/lib/python3.11/site-packages/langgraph/graph/graph.py:36: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langgraph.pregel import Channel, Pregel\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import nest_asyncio\n",
    "import operator\n",
    "import functools\n",
    "from pathlib import Path\n",
    "import tiktoken\n",
    "from typing import TypedDict, Annotated, List, Optional, Sequence, Any, Callable, Union, Dict\n",
    "from operator import itemgetter\n",
    "import getpass\n",
    "\n",
    "# LangChain Imports\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.runnables import Runnable, RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangGraph Imports\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# ArXiv and Document Processing\n",
    "import arxiv\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using working directory: content/data/1c492203\n"
     ]
    }
   ],
   "source": [
    "# ==================== UTILITY FUNCTIONS ====================\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    \"\"\"Calculate the token length using tiktoken.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def extract_arxiv_id(url_or_id):\n",
    "    \"\"\"Extracts the core ArXiv ID without the version suffix from a URL or ID string.\"\"\"\n",
    "    if not url_or_id:\n",
    "        return None\n",
    "    match = re.search(r'(\\d+\\.\\d+)', url_or_id)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def create_random_subdirectory():\n",
    "    \"\"\"Create a random subdirectory for storing files.\"\"\"\n",
    "    random_id = str(uuid.uuid4())[:8]  # Use first 8 characters of a UUID\n",
    "    subdirectory_path = os.path.join('./content/data', random_id)\n",
    "    os.makedirs(subdirectory_path, exist_ok=True)\n",
    "    return subdirectory_path\n",
    "\n",
    "# Create the working directory\n",
    "os.makedirs('./content/data', exist_ok=True)\n",
    "WORKING_DIRECTORY = Path(create_random_subdirectory())\n",
    "print(f\"Using working directory: {WORKING_DIRECTORY}\")\n",
    "\n",
    "# Default platform - this can be changed as needed\n",
    "PLATFORM = \"LinkedIn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ARXIV RAG TOOL ====================\n",
    "\n",
    "class ArxivRAGInput(BaseModel):\n",
    "    \"\"\"Input schema for the ArXiv RAG tool.\"\"\"\n",
    "    domain: str = Field(description=\"The research domain or topic to search for papers on ArXiv\")\n",
    "    question: str = Field(description=\"The specific question to answer based *only* on the content of the retrieved ArXiv papers.\")\n",
    "    max_papers: Optional[int] = Field(default=1, description=\"Maximum number of relevant papers to download and process.\")\n",
    "\n",
    "@tool(\"arxiv_rag_tool\", args_schema=ArxivRAGInput)\n",
    "def arxiv_rag_tool_func(domain: str, question: str, max_papers: int = 1) -> str:\n",
    "    \"\"\"ArXiv RAG Tool for searching, retrieving and answering questions based on ArXiv papers.\"\"\"\n",
    "    global rag_chain, qdrant_retriever\n",
    "    \n",
    "    print(f\"\\n=== Executing ArXiv RAG Tool ===\\n\")\n",
    "    print(f\"Domain: {domain}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Max Papers: {max_papers}\")\n",
    "\n",
    "    # 1. Search ArXiv for relevant paper IDs and URLs\n",
    "    print(\"Searching ArXiv...\")\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        search = arxiv.Search(\n",
    "            query=domain,\n",
    "            max_results=max_papers,\n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "        results = list(client.results(search))\n",
    "    except Exception as e:\n",
    "        return f\"Error searching ArXiv: {e}\"\n",
    "\n",
    "    if not results:\n",
    "        return f\"No papers found on ArXiv for the domain: '{domain}'\"\n",
    "\n",
    "    papers_metadata = []\n",
    "    for result in results:\n",
    "        arxiv_id = extract_arxiv_id(result.entry_id)\n",
    "        pdf_url = result.pdf_url\n",
    "        if arxiv_id and pdf_url:\n",
    "            papers_metadata.append({\n",
    "                \"title\": result.title,\n",
    "                \"arxiv_id\": arxiv_id,\n",
    "                \"pdf_url\": pdf_url,\n",
    "                \"summary\": result.summary.replace('\\n', ' ')\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping paper '{result.title}' due to missing ID or PDF URL.\")\n",
    "\n",
    "    if not papers_metadata:\n",
    "        return f\"Found papers for '{domain}', but none had valid metadata (ID and PDF URL).\"\n",
    "\n",
    "    print(f\"Found {len(papers_metadata)} papers with valid metadata.\")\n",
    "\n",
    "    # 2. Download and Load Documents\n",
    "    all_docs = []\n",
    "    print(\"Downloading and loading papers...\")\n",
    "    for i, paper in enumerate(papers_metadata):\n",
    "        print(f\"Loading paper {i+1}/{len(papers_metadata)}: {paper['title']} ({paper['arxiv_id']})\")\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(paper['pdf_url'])\n",
    "            docs = loader.load()\n",
    "            # Add metadata to each page\n",
    "            for doc in docs:\n",
    "                doc.metadata.update({\n",
    "                    \"source\": f\"{paper['title']} (ID: {paper['arxiv_id']})\",\n",
    "                    \"arxiv_id\": paper['arxiv_id'],\n",
    "                    \"title\": paper['title'],\n",
    "                })\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading paper {paper['title']}: {e}. Skipping.\")\n",
    "\n",
    "    if not all_docs:\n",
    "        return \"Failed to download or load content from the found ArXiv papers.\"\n",
    "\n",
    "    print(f\"Loaded a total of {len(all_docs)} pages.\")\n",
    "\n",
    "    # 3. Chunk Documents\n",
    "    print(\"Splitting documents into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200, \n",
    "        chunk_overlap=20,\n",
    "        length_function=tiktoken_len\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(all_docs)\n",
    "    print(f\"Split into {len(chunks)} chunks.\")\n",
    "\n",
    "    if not chunks:\n",
    "        return \"Failed to split documents into chunks.\"\n",
    "\n",
    "    # 4. Create Vector Store and update global RAG chain\n",
    "    print(\"Creating vector store...\")\n",
    "    try:\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        vectorstore = Qdrant.from_documents(\n",
    "            chunks,\n",
    "            embeddings,\n",
    "            location=\":memory:\",\n",
    "            collection_name=\"arxiv_rag_collection\"\n",
    "        )\n",
    "        qdrant_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "        rag_chain = create_rag_chain(qdrant_retriever)\n",
    "        print(\"Vector store created and global RAG chain updated.\")\n",
    "    except Exception as e:\n",
    "        return f\"Error creating vector store: {e}\"\n",
    "\n",
    "    # 5. Retrieve and Generate Answer\n",
    "    print(f\"Answering question: {question}\")\n",
    "    try:\n",
    "        final_answer = rag_chain.invoke({\"question\": question})     \n",
    "        print(f\"--- ArXiv RAG Tool Execution Finished ---\")\n",
    "        return final_answer\n",
    "    except Exception as e:\n",
    "        print(f\"--- ArXiv RAG Tool Execution Failed ---\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        return f\"Error generating answer with RAG chain: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== FILE OPERATION TOOLS ====================\n",
    "\n",
    "@tool\n",
    "def create_outline(\n",
    "    points: Annotated[List[str], \"List of main points or sections.\"],\n",
    "    file_name: Annotated[str, \"File path to save the outline.\"],\n",
    ") -> Annotated[str, \"Path of the saved outline file.\"]:\n",
    "    \"\"\"Create and save an outline.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        for i, point in enumerate(points):\n",
    "            file.write(f\"{i + 1}. {point}\\n\")\n",
    "    return f\"Outline saved to {file_name}\"\n",
    "\n",
    "@tool\n",
    "def read_document(\n",
    "    file_name: Annotated[str, \"File path to save the document.\"],\n",
    "    start: Annotated[Optional[int], \"The start line. Default is 0\"] = None,\n",
    "    end: Annotated[Optional[int], \"The end line. Default is None\"] = None,\n",
    ") -> str:\n",
    "    \"\"\"Read the specified document.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    if start is not None and start == 0:\n",
    "        start = 0\n",
    "    return \"\\n\".join(lines[start:end])\n",
    "\n",
    "@tool\n",
    "def write_document(\n",
    "    content: Annotated[str, \"Text content to be written into the document.\"],\n",
    "    file_name: Annotated[str, \"File path to save the document.\"],\n",
    ") -> Annotated[str, \"Path of the saved document file.\"]:\n",
    "    \"\"\"Create and save a text document.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        file.write(content)\n",
    "    return f\"Document saved to {file_name}\"\n",
    "\n",
    "@tool\n",
    "def edit_document(\n",
    "    file_name: Annotated[str, \"Path of the document to be edited.\"],\n",
    "    inserts: Annotated[\n",
    "        Dict[int, str],\n",
    "        \"Dictionary where key is the line number (1-indexed) and value is the text to be inserted at that line.\",\n",
    "    ] = {},\n",
    ") -> Annotated[str, \"Path of the edited document file.\"]:\n",
    "    \"\"\"Edit a document by inserting text at specific line numbers.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    sorted_inserts = sorted(inserts.items())\n",
    "    for line_number, text in sorted_inserts:\n",
    "        if 1 <= line_number <= len(lines) + 1:\n",
    "            lines.insert(line_number - 1, text + \"\\n\")\n",
    "        else:\n",
    "            return f\"Error: Line number {line_number} is out of range.\"\n",
    "\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "    return f\"Document edited and saved to {file_name}\"\n",
    "\n",
    "@tool\n",
    "def retrieve_information(\n",
    "    query: Annotated[str, \"query to ask the retrieve information tool\"]\n",
    "    ):\n",
    "    \"\"\"Use Retrieval Augmented Generation to retrieve information about the paper.\"\"\"\n",
    "    # This will be initialized when the LLM is loaded\n",
    "    return rag_chain.invoke({\"question\": query})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== AGENT CREATION UTILITIES ====================\n",
    "\n",
    "def create_agent(\n",
    "    llm: ChatOpenAI,\n",
    "    tools: list,\n",
    "    system_prompt: str,\n",
    ") -> AgentExecutor:\n",
    "    \"\"\"Create a function-calling agent with the given LLM, tools, and system prompt.\"\"\"\n",
    "    system_prompt += (\"\\nWork autonomously according to your specialty, using the tools available to you.\"\n",
    "    \" Do not ask for clarification.\"\n",
    "    \" Your other team members (and other teams) will collaborate with you with their own specialties.\"\n",
    "    \" You are chosen for a reason! You are one of the following team members: {team_members}.\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor\n",
    "\n",
    "def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> Runnable:\n",
    "    \"\"\"Create an LLM-based routing supervisor.\"\"\"\n",
    "    options = [\"FINISH\"] + members\n",
    "\n",
    "    function_def = {\n",
    "        \"name\": \"route\",\n",
    "        \"description\": \"Select the next role.\",\n",
    "        \"parameters\": {\n",
    "            \"title\": \"routeSchema\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"next\": {\n",
    "                    \"title\": \"Next\",\n",
    "                    \"anyOf\": [\n",
    "                        {\"enum\": options},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"next\"],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Given the conversation above, who should act next?\"\n",
    "                \" Or should we FINISH? Select one of: {options}\",\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(options=str(options), team_members=\", \".join(members))\n",
    "    \n",
    "    return (\n",
    "        prompt\n",
    "        | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "        | JsonOutputFunctionsParser()\n",
    "    )\n",
    "\n",
    "def prelude(state):\n",
    "    \"\"\"Get current file information to provide context to agents.\"\"\"\n",
    "    written_files = []\n",
    "    if not WORKING_DIRECTORY.exists():\n",
    "        WORKING_DIRECTORY.mkdir()\n",
    "    try:\n",
    "        written_files = [\n",
    "            f.relative_to(WORKING_DIRECTORY) for f in WORKING_DIRECTORY.rglob(\"*\")\n",
    "        ]\n",
    "    except:\n",
    "        pass\n",
    "    if not written_files:\n",
    "        return {**state, \"current_files\": \"No files written.\"}\n",
    "    return {\n",
    "        **state,\n",
    "        \"current_files\": \"\\nBelow are files your team has written to the directory:\\n\"\n",
    "        + \"\\n\".join([f\" - {f}\" for f in written_files]),\n",
    "    }\n",
    "\n",
    "def prelude_verification(state):\n",
    "    \"\"\"Get current file information with platform information.\"\"\"\n",
    "    base_state = prelude(state) \n",
    "    base_state['target_platform'] = state.get('target_platform', PLATFORM)\n",
    "    return base_state\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    \"\"\"Standard agent node function that adds agent name to messages.\"\"\"\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n",
    "\n",
    "def route_next(x):\n",
    "    \"\"\"Extract the next agent from the supervisor's output and normalize case.\"\"\"\n",
    "    next_step = x[\"next\"]\n",
    "    # Normalize case to prevent 'FinISH' vs 'FINISH' errors\n",
    "    if isinstance(next_step, str) and next_step.upper() == \"FINISH\":\n",
    "        return \"FINISH\"\n",
    "    return next_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STATE DEFINITIONS ====================\n",
    "\n",
    "# Base state for all teams\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    next: str\n",
    "\n",
    "# Research team state\n",
    "class ResearchTeamState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    team_members: List[str] \n",
    "    next: str\n",
    "\n",
    "# Authoring team state\n",
    "class DocWritingState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    team_members: str\n",
    "    next: str\n",
    "    current_files: str\n",
    "\n",
    "# Verification team state\n",
    "class VerificationState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    team_members: str\n",
    "    next: str\n",
    "    current_files: str\n",
    "    target_platform: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== INITIALIZE LLMs ====================\n",
    "\n",
    "# Use consistent LLM instances\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm_tools = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Initialize tools that need LLM\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "tool_belt = [arxiv_rag_tool_func]\n",
    "tool_node = ToolNode(tool_belt)\n",
    "model_with_tools = llm_tools.bind_tools(tool_belt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== DEFINE GLOBAL RAG CHAIN ====================\n",
    "# Define a global RAG chain for use across tools\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Formats retrieved documents into a single string for the prompt context.\"\"\"\n",
    "    if not docs:\n",
    "        return \"No relevant context found in the documents.\"\n",
    "    context_str = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"Source: {doc.metadata.get('source', 'Unknown')}\\nContent: {doc.page_content}\"\n",
    "        for doc in docs\n",
    "    ])\n",
    "    return context_str\n",
    "\n",
    "# Initialize empty vectorstore and retriever placeholder\n",
    "qdrant_retriever = None\n",
    "\n",
    "# Prompt template for RAG\n",
    "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Answer the question based ONLY on the provided context from the ArXiv papers.\n",
    "If the context doesn't contain the answer, state that the information is not available in the retrieved documents.\n",
    "Cite the source document title(s) if possible based on the context metadata.\n",
    "\"\"\")\n",
    "\n",
    "# Define the RAG chain (will use the retriever once it's created)\n",
    "def create_rag_chain(retriever):\n",
    "    return (\n",
    "        {\n",
    "            \"context\": itemgetter(\"question\") | retriever | RunnableLambda(format_docs),\n",
    "            \"question\": itemgetter(\"question\")\n",
    "        }\n",
    "        | rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "# Initialize with a dummy chain - will be replaced when vectorstore is created\n",
    "dummy_response = \"RAG chain not initialized yet. Please run ArXiv search first.\"\n",
    "rag_chain = RunnablePassthrough() | (lambda _: dummy_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/kl800c1j6hjc9xt9lm_1bhph0000gn/T/ipykernel_95480/2573590491.py:64: LangChainDeprecationWarning: The method `BaseChatOpenAI.bind_functions` was deprecated in langchain-openai 0.2.1 and will be removed in 1.0.0. Use :meth:`~langchain_openai.chat_models.base.ChatOpenAI.bind_tools` instead.\n",
      "  | llm.bind_functions(functions=[function_def], function_call=\"route\")\n"
     ]
    }
   ],
   "source": [
    "# ==================== RESEARCH TEAM ====================\n",
    "\n",
    "# Search Agent - IMPROVED INSTRUCTIONS\n",
    "search_agent = create_agent(\n",
    "    llm,\n",
    "    [tavily_tool],\n",
    "    \"You are a research assistant who searches for up-to-date info using the tavily search engine. \"\n",
    "    \"Your job is ONLY to find factual information about the paper - NOT to write content. \"\n",
    "    \"Focus on finding: title, authors, publication date, key findings, methodology. \"\n",
    "    \"Present information in a structured format with clear headings.\"\n",
    ")\n",
    "search_node = functools.partial(agent_node, agent=search_agent, name=\"Search\")\n",
    "\n",
    "# Research Agent\n",
    "research_agent = create_agent(\n",
    "    llm,\n",
    "    [retrieve_information],\n",
    "    \"You are a research assistant who can provide specific information on scientific papers. \"\n",
    "    \"You must only respond with factual information about the paper related to the request. \"\n",
    "    \"Do NOT create a draft post or final content - just provide structured research findings.\"\n",
    ")\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"PaperInformationRetriever\")\n",
    "\n",
    "# Research Team Supervisor\n",
    "research_supervisor = create_team_supervisor(\n",
    "    llm,\n",
    "    (\"You are a supervisor tasked with managing a research team consisting of: Search, PaperInformationRetriever. \"\n",
    "     \"Your team's ONLY responsibility is to gather factual information about the requested paper. \"\n",
    "     \"DO NOT create a draft post - that will be handled by a separate team. \"\n",
    "     \"First, use Search to find basic information about the paper. \"\n",
    "     \"Then use PaperInformationRetriever to get detailed information. \"\n",
    "     \"IMPORTANT: If Search cannot find information, try PaperInformationRetriever directly. \"\n",
    "     \"DO NOT FINISH until you have ACTUAL research content. If team members report they 'cannot access' \"\n",
    "     \"or 'unable to retrieve' information, that means research is NOT complete. \"\n",
    "     \"Only respond with FINISH when you have collected real information about the paper.\"),\n",
    "    [\"Search\", \"PaperInformationRetriever\"]\n",
    ")\n",
    "\n",
    "# Research Team Graph\n",
    "research_graph = StateGraph(ResearchTeamState)\n",
    "research_graph.add_node(\"Search\", search_node)\n",
    "research_graph.add_node(\"PaperInformationRetriever\", research_node)\n",
    "research_graph.add_node(\"supervisor\", research_supervisor)\n",
    "\n",
    "# Add edges\n",
    "research_graph.add_edge(\"Search\", \"supervisor\")\n",
    "research_graph.add_edge(\"PaperInformationRetriever\", \"supervisor\")\n",
    "research_graph.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    route_next,\n",
    "    {\"Search\": \"Search\", \"PaperInformationRetriever\": \"PaperInformationRetriever\", \"FINISH\": END}\n",
    ")\n",
    "research_graph.set_entry_point(\"supervisor\")\n",
    "compiled_research_graph = research_graph.compile()\n",
    "\n",
    "# Research Chain\n",
    "def enter_research_chain(message: str):\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "    }\n",
    "\n",
    "research_chain = enter_research_chain | compiled_research_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== PLATFORM-AGNOSTIC AUTHORING TEAM ====================\n",
    "\n",
    "# Note Taker Agent\n",
    "agnostic_note_taker_agent = create_agent(\n",
    "    llm, \n",
    "    [create_outline, read_document],\n",
    "    (\"You are a senior researcher creating a factual outline based on the research findings provided. \"\n",
    "     \"Create a detailed outline that captures the key points about the paper. \"\n",
    "     \"This outline will be used by the document writer to create the full post. \"\n",
    "     \"Save your outline as 'post_outline.md'.\\n\"\n",
    "     \"Below are files currently in your directory:\\n{current_files}\")\n",
    ")\n",
    "context_aware_agnostic_note_taker = prelude | agnostic_note_taker_agent\n",
    "agnostic_note_taking_node = functools.partial(\n",
    "    agent_node, \n",
    "    agent=context_aware_agnostic_note_taker, \n",
    "    name=\"AgnosticNoteTaker\"\n",
    ")\n",
    "\n",
    "# Document Writer Agent - IMPROVED INSTRUCTIONS\n",
    "agnostic_doc_writer_agent = create_agent(\n",
    "    llm, \n",
    "    [write_document, edit_document, read_document],\n",
    "    (\"You are a technical writer focused on creating clear, accurate, and platform-agnostic content. \"\n",
    "     \"IMPORTANT: You MUST save your final draft to a file named 'draft_social_post.md'. \"\n",
    "     \"This specific filename is critical as other teams depend on it. \"\n",
    "     \"Create a well-structured post based on the research findings. \"\n",
    "     \"Below are files currently in your directory:\\n{current_files}\")\n",
    ")\n",
    "context_aware_agnostic_doc_writer = prelude | agnostic_doc_writer_agent\n",
    "agnostic_doc_writing_node = functools.partial(\n",
    "    agent_node, \n",
    "    agent=context_aware_agnostic_doc_writer, \n",
    "    name=\"AgnosticDocWriter\"\n",
    ")\n",
    "\n",
    "# Copy Editor Agent\n",
    "agnostic_copy_editor_agent = create_agent(\n",
    "    llm, \n",
    "    [read_document, edit_document],\n",
    "    (\"You are an expert copy editor who focuses on fixing grammar, spelling, and tone issues. \"\n",
    "     \"Read the draft post and make necessary edits to improve readability and correctness. \"\n",
    "     \"The final edited version must remain in the file 'draft_social_post.md'. \"\n",
    "     \"Below are files currently in your directory:\\n{current_files}\")\n",
    ")\n",
    "context_aware_agnostic_copy_editor = prelude | agnostic_copy_editor_agent\n",
    "agnostic_copy_editing_node = functools.partial(\n",
    "    agent_node, \n",
    "    agent=context_aware_agnostic_copy_editor, \n",
    "    name=\"AgnosticCopyEditor\"\n",
    ")\n",
    "\n",
    "# Supervisor for the Agnostic Authoring Team\n",
    "agnostic_authoring_supervisor = create_team_supervisor(\n",
    "    llm,\n",
    "    (\"You are a supervisor tasked with managing content creation for a scientific paper post. \"\n",
    "     \"Your team members are: {team_members}. \"\n",
    "     \"Follow this precise workflow:\"\n",
    "     \"1. First, route to AgnosticNoteTaker to create an outline.\"\n",
    "     \"2. Next, route to AgnosticDocWriter to write the full post.\"\n",
    "     \"3. Finally, route to AgnosticCopyEditor to refine the post.\"\n",
    "     \"CRITICAL: Ensure the AgnosticDocWriter saves the post as 'draft_social_post.md'.\"\n",
    "     \"After all team members have completed their work, respond with FINISH.\"),\n",
    "    [\"AgnosticNoteTaker\", \"AgnosticDocWriter\", \"AgnosticCopyEditor\"]\n",
    ")\n",
    "\n",
    "# Graph for the Agnostic Authoring Team\n",
    "agnostic_authoring_graph = StateGraph(DocWritingState)\n",
    "agnostic_authoring_graph.add_node(\"AgnosticNoteTaker\", agnostic_note_taking_node)\n",
    "agnostic_authoring_graph.add_node(\"AgnosticDocWriter\", agnostic_doc_writing_node)\n",
    "agnostic_authoring_graph.add_node(\"AgnosticCopyEditor\", agnostic_copy_editing_node)\n",
    "agnostic_authoring_graph.add_node(\"supervisor\", agnostic_authoring_supervisor)\n",
    "\n",
    "# Add edges\n",
    "agnostic_authoring_graph.add_edge(\"AgnosticNoteTaker\", \"supervisor\")\n",
    "agnostic_authoring_graph.add_edge(\"AgnosticDocWriter\", \"supervisor\")\n",
    "agnostic_authoring_graph.add_edge(\"AgnosticCopyEditor\", \"supervisor\")\n",
    "\n",
    "agnostic_authoring_graph.add_conditional_edges(\n",
    "    \"supervisor\", \n",
    "    route_next,\n",
    "    {\n",
    "        \"AgnosticNoteTaker\": \"AgnosticNoteTaker\", \n",
    "        \"AgnosticDocWriter\": \"AgnosticDocWriter\",\n",
    "        \"AgnosticCopyEditor\": \"AgnosticCopyEditor\", \n",
    "        \"FINISH\": END\n",
    "    }\n",
    ")\n",
    "agnostic_authoring_graph.set_entry_point(\"supervisor\")\n",
    "compiled_agnostic_authoring_graph = agnostic_authoring_graph.compile()\n",
    "\n",
    "# Authoring Chain\n",
    "def enter_agnostic_authoring_chain(message: str):\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "        \"team_members\": \", \".join([\"AgnosticNoteTaker\", \"AgnosticDocWriter\", \"AgnosticCopyEditor\"])\n",
    "    }\n",
    "\n",
    "agnostic_authoring_chain = enter_agnostic_authoring_chain | compiled_agnostic_authoring_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VERIFICATION TEAM ====================\n",
    "\n",
    "# Fact Checker Agent\n",
    "fact_checker_agent = create_agent(\n",
    "    llm,\n",
    "    [read_document], \n",
    "    (\"You are a meticulous fact-checker. Your task is to read the draft social media post \"\n",
    "     \"in 'draft_social_post.md'. If this file doesn't exist, report that it's missing. \"\n",
    "     \"Compare the post content against the research findings provided earlier. \"\n",
    "     \"Ensure all claims are accurate and supported by the research. \"\n",
    "     \"Be specific in your response - state 'FACT CHECK: APPROVED' if accurate, \"\n",
    "     \"or 'FACT CHECK: ISSUES FOUND' followed by the specific inaccuracies.\"\n",
    "     \"\\nBelow are files currently in your directory:\\n{current_files}\")\n",
    ")\n",
    "context_aware_fact_checker = prelude | fact_checker_agent\n",
    "fact_checking_node = functools.partial(\n",
    "    agent_node, \n",
    "    agent=context_aware_fact_checker,\n",
    "    name=\"FactChecker\"\n",
    ")\n",
    "\n",
    "# Style Validator Agent\n",
    "style_validator_agent = create_agent(\n",
    "    llm,\n",
    "    [read_document],\n",
    "    (\"You are a social media expert. Your task is to read the draft post \"\n",
    "     \"in 'draft_social_post.md'. If this file doesn't exist, report that it's missing. \"\n",
    "     \"Verify that its tone, style, length, and formatting are appropriate for {target_platform}. \"\n",
    "     \"Be specific in your response - state 'STYLE CHECK: APPROVED' if it fits the platform, \"\n",
    "     \"or 'STYLE CHECK: ISSUES FOUND' followed by the specific style issues.\"\n",
    "     \"\\nBelow are files currently in your directory:\\n{current_files}\")\n",
    ")\n",
    "context_aware_style_validator = prelude_verification | style_validator_agent\n",
    "style_validation_node = functools.partial(\n",
    "    agent_node, \n",
    "    agent=context_aware_style_validator, \n",
    "    name=\"StyleValidator\"\n",
    ")\n",
    "\n",
    "# Verification Team Supervisor\n",
    "verification_supervisor = create_team_supervisor(\n",
    "    llm,\n",
    "    (\"You are a supervisor for the Verification Team. Your team consists of: {team_members}. \"\n",
    "     \"Your goal is to ensure the draft post in 'draft_social_post.md' is factually correct and \"\n",
    "     \"stylistically appropriate for {target_platform}. \"\n",
    "     \"First, route to the FactChecker. Then, only after the FactChecker has completed, \"\n",
    "     \"route to the StyleValidator. \"\n",
    "     \"After BOTH agents have approved the post, respond with FINISH. \"\n",
    "     \"If any issues are found, report them and then respond with FINISH.\"),\n",
    "    [\"FactChecker\", \"StyleValidator\"]\n",
    ")\n",
    "\n",
    "# Verification Graph\n",
    "verification_graph = StateGraph(VerificationState)\n",
    "verification_graph.add_node(\"FactChecker\", fact_checking_node)\n",
    "verification_graph.add_node(\"StyleValidator\", style_validation_node)\n",
    "verification_graph.add_node(\"supervisor\", verification_supervisor)\n",
    "\n",
    "# Add edges\n",
    "verification_graph.add_edge(\"FactChecker\", \"supervisor\")\n",
    "verification_graph.add_edge(\"StyleValidator\", \"supervisor\")\n",
    "verification_graph.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    route_next,\n",
    "    {\n",
    "        \"FactChecker\": \"FactChecker\",\n",
    "        \"StyleValidator\": \"StyleValidator\",\n",
    "        \"FINISH\": END\n",
    "    }\n",
    ")\n",
    "verification_graph.set_entry_point(\"supervisor\")\n",
    "compiled_verification_graph = verification_graph.compile()\n",
    "\n",
    "def enter_verification_chain(state: State):\n",
    "    \"\"\"Prepare the state for the verification team.\"\"\"\n",
    "    last_message = state['messages'][-1]\n",
    "    \n",
    "    # Check for specific files we expect\n",
    "    written_files = [f.relative_to(WORKING_DIRECTORY) for f in WORKING_DIRECTORY.rglob(\"*\")]\n",
    "    post_file_exists = any(\"draft_social_post.md\" in str(f) for f in written_files)\n",
    "    \n",
    "    current_files_str = \"\\n\".join([f\" - {f}\" for f in written_files]) if written_files else \"No files found.\"\n",
    "    \n",
    "    # Add information about expected files\n",
    "    if not post_file_exists:\n",
    "        current_files_str += \"\\n\\nWARNING: Expected file 'draft_social_post.md' not found!\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [last_message], \n",
    "        \"team_members\": \"FactChecker, StyleValidator\", \n",
    "        \"target_platform\": PLATFORM,\n",
    "        \"current_files\": current_files_str\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== TOP-LEVEL SUPERVISOR AND GRAPH ====================\n",
    "\n",
    "# Top-level Supervisor\n",
    "top_level_supervisor = create_team_supervisor(\n",
    "    llm,\n",
    "    (\"You are a top-level supervisor managing a multi-team workflow to create posts about scientific papers. \"\n",
    "     \"You MUST follow this EXACT sequence in order WITHOUT SKIPPING ANY TEAM: \"\n",
    "     \"1. FIRST: 'Research team' gathers facts about the paper. \"\n",
    "     \"2. NEXT: 'Agnostic Authoring team' creates a draft post saved to 'draft_social_post.md'. \"\n",
    "     \"3. FINALLY: 'Verification team' checks the draft. \"\n",
    "     \"\\n\\nSTRICTLY ENFORCE this sequence. You CANNOT skip any team. \"\n",
    "     \"You MUST NOT route to 'FINISH' until ALL THREE teams have completed their work in sequence. \"\n",
    "     \"Even if a team reports problems, you must complete the entire sequence.\"),\n",
    "    [\"Research team\", \"Agnostic Authoring team\", \"Verification team\"]\n",
    ")\n",
    "\n",
    "# Node functions for super graph\n",
    "def run_research_chain(state: State):\n",
    "    \"\"\"Run the research chain with the last message.\"\"\"\n",
    "    last_message_content = state['messages'][-1].content\n",
    "    result = research_chain.invoke(last_message_content)\n",
    "    return {\"messages\": result[\"messages\"]}\n",
    "\n",
    "def run_agnostic_authoring_chain(state: State):\n",
    "    \"\"\"Run the agnostic authoring chain with the last message.\"\"\"\n",
    "    last_message_content = state['messages'][-1].content\n",
    "    result = agnostic_authoring_chain.invoke(last_message_content)\n",
    "    return {\"messages\": result[\"messages\"]}\n",
    "\n",
    "# Main Workflow Graph\n",
    "super_graph = StateGraph(State)\n",
    "super_graph.add_node(\"Research team\", run_research_chain)\n",
    "super_graph.add_node(\"Agnostic Authoring team\", run_agnostic_authoring_chain)\n",
    "\n",
    "# For verification, we chain preparation and execution\n",
    "verification_node_chain = enter_verification_chain | compiled_verification_graph\n",
    "super_graph.add_node(\"Verification team\", verification_node_chain)\n",
    "\n",
    "super_graph.add_node(\"supervisor\", top_level_supervisor)\n",
    "\n",
    "# Add edges\n",
    "super_graph.add_edge(\"Research team\", \"supervisor\")\n",
    "super_graph.add_edge(\"Agnostic Authoring team\", \"supervisor\")\n",
    "super_graph.add_edge(\"Verification team\", \"supervisor\")\n",
    "\n",
    "# Add conditional edges\n",
    "super_graph.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    route_next,\n",
    "    {\n",
    "        \"Research team\": \"Research team\",\n",
    "        \"Agnostic Authoring team\": \"Agnostic Authoring team\",\n",
    "        \"Verification team\": \"Verification team\",\n",
    "        \"FINISH\": END\n",
    "    }\n",
    ")\n",
    "super_graph.set_entry_point(\"supervisor\")\n",
    "compiled_super_graph = super_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== EXECUTION ====================\n",
    "\n",
    "def run_workflow(query, platform=None):\n",
    "    \"\"\"Run the entire workflow with the given query and improved debugging.\"\"\"\n",
    "    global WORKING_DIRECTORY, PLATFORM\n",
    "    \n",
    "    # Update platform if specified\n",
    "    if platform is not None:\n",
    "        PLATFORM = platform\n",
    "    \n",
    "    # Create a new working directory for this run\n",
    "    WORKING_DIRECTORY = Path(create_random_subdirectory())\n",
    "    print(f\"Using working directory: {WORKING_DIRECTORY}\")\n",
    "    \n",
    "    print(\"Starting graph execution...\")\n",
    "    \n",
    "    initial_message = HumanMessage(\n",
    "        content=f\"Write a post about the paper '{query}'. \"\n",
    "                f\"Use the standard process: Research, create a neutral draft, and then verify it \"\n",
    "                f\"for factual accuracy and suitability for the {PLATFORM} platform. Save the final post.\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Add step tracking for debugging\n",
    "        current_step = \"Initializing\"\n",
    "        step_completion = {\n",
    "            \"Research team\": False,\n",
    "            \"Agnostic Authoring team\": False,\n",
    "            \"Verification team\": False\n",
    "        }\n",
    "        \n",
    "        for s in compiled_super_graph.stream(\n",
    "            {\n",
    "                \"messages\": [initial_message],\n",
    "            },\n",
    "            {\"recursion_limit\": 50},\n",
    "        ):\n",
    "            if \"__end__\" not in s:\n",
    "                # Track steps for debugging\n",
    "                if \"supervisor\" in s:\n",
    "                    next_step = s[\"supervisor\"].get(\"next\", \"UNKNOWN\")\n",
    "                    if next_step != \"FINISH\":\n",
    "                        current_step = next_step\n",
    "                        print(f\"\\n--- MOVING TO: {current_step} ---\\n\")\n",
    "                    else:\n",
    "                        print(f\"\\n--- WORKFLOW ATTEMPTING TO FINISH ---\\n\")\n",
    "                        print(f\"Step completion status: {step_completion}\")\n",
    "                \n",
    "                # Mark steps as complete when we see their output\n",
    "                for team in step_completion.keys():\n",
    "                    if team in s:\n",
    "                        step_completion[team] = True\n",
    "                        print(f\"Completed: {team}\")\n",
    "                \n",
    "                print(s)\n",
    "                print(\"---\")\n",
    "        \n",
    "        print(\"Graph execution finished.\")\n",
    "        print(f\"Final step completion status: {step_completion}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(\"\\nAn error occurred during graph execution:\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    # List and read files\n",
    "    print(\"\\nFiles in working directory:\")\n",
    "    found_post = False\n",
    "    for item in WORKING_DIRECTORY.iterdir():\n",
    "        print(f\"- {item.name}\")\n",
    "        if item.name == \"draft_social_post.md\":\n",
    "            found_post = True\n",
    "            print(\"\\nFinal post content:\")\n",
    "            with open(item, \"r\") as f:\n",
    "                print(f.read())\n",
    "    \n",
    "    if not found_post:\n",
    "        print(\"\\nWARNING: draft_social_post.md was not created!\")\n",
    "        \n",
    "    return found_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using working directory: content/data/5e9ac346\n",
      "Starting graph execution...\n",
      "\n",
      "--- MOVING TO: Research team ---\n",
      "\n",
      "{'supervisor': {'next': 'Research team'}}\n",
      "---\n",
      "Completed: Research team\n",
      "{'Research team': {'messages': [HumanMessage(content=\"Write a post about the paper 'Extending Llama-3's Context Ten-Fold Overnight'. Use the standard process: Research, create a neutral draft, and then verify it for factual accuracy and suitability for the LinkedIn platform. Save the final post.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"### Title\\n**Extending Llama-3's Context Ten-Fold Overnight**\\n\\n### Authors\\nThe specific authors are not listed in the search results. Further reviews or the original paper may provide this information.\\n\\n### Publication Date\\n**April 30, 2024**\\n\\n### Key Findings\\n- The research successfully extended the context length of the Llama-3-8B-Instruct model from **8,000 tokens to 80,000 tokens**.\\n- This extension enables the model to process and understand significantly longer pieces of text.\\n- The methodology demonstrated superior performance across various evaluation tasks such as NIHS, topic retrieval, and long-context language understanding while retaining its original capabilities for short contexts.\\n\\n### Methodology\\n- The researchers utilized a technique called **Quantized Low-Rank Adaptation (QLoRA)** for fine-tuning.\\n- The entire training process was completed in **only 8 hours** using a single powerful **8xA800 (80G) GPU machine**.\\n- QLoRA allows for efficient updates to model parameters while significantly reducing the memory footprint during the fine-tuning phase by quantizing the pre-trained model to **4-bit precision**. \\n\\nFor further details, you can refer to the original source of the paper or the linked articles:\\n- [DEV Community](https://dev.to/aimodels-fyi/extending-llama-3s-context-ten-fold-overnight-4340)\\n- [AIModels.fyi](https://www.aimodels.fyi/papers/arxiv/extending-llama-3s-context-ten-fold-overnight)\\n- [Hugging Face](https://huggingface.co/papers/2404.19553)\", additional_kwargs={}, response_metadata={}, name='Search'), HumanMessage(content=\"### Title\\n**Extending Llama-3's Context Ten-Fold Overnight**\\n\\n### Authors\\nThe specific authors are not listed in the information retrieved.\\n\\n### Publication Date\\n**April 30, 2024**\\n\\n### Key Findings\\n- The study achieved an extension of the Llama-3-8B-Instruct model's context length from **8,000 tokens to 80,000 tokens**.\\n- This enhancement allows the model to better process and comprehend significantly longer text passages.\\n- Superior performance was observed across various evaluation metrics including NIHS, topic retrieval tasks, and long-context language understanding, while the model retained its effectiveness with shorter contexts.\\n\\n### Methodology\\n- The researchers implemented **Quantized Low-Rank Adaptation (QLoRA)** for the fine-tuning process.\\n- The training was executed in **just 8 hours** utilizing a robust **8xA800 (80G) GPU machine**.\\n- QLoRA enabled efficient updates to model parameters by quantizing the pre-trained model to **4-bit precision**, which significantly reduced memory usage during training.\\n\\nFor further details, please refer to the original source or linked articles:\\n- [DEV Community](https://dev.to/aimodels-fyi/extending-llama-3s-context-ten-fold-overnight-4340)\\n- [AIModels.fyi](https://www.aimodels.fyi/papers/arxiv/extending-llama-3s-context-ten-fold-overnight)\\n- [Hugging Face](https://huggingface.co/papers/2404.19553)\", additional_kwargs={}, response_metadata={}, name='PaperInformationRetriever')]}}\n",
      "---\n",
      "\n",
      "--- MOVING TO: Agnostic Authoring team ---\n",
      "\n",
      "{'supervisor': {'next': 'Agnostic Authoring team'}}\n",
      "---\n",
      "Completed: Agnostic Authoring team\n",
      "{'Agnostic Authoring team': {'messages': [HumanMessage(content=\"### Title\\n**Extending Llama-3's Context Ten-Fold Overnight**\\n\\n### Authors\\nThe specific authors are not listed in the information retrieved.\\n\\n### Publication Date\\n**April 30, 2024**\\n\\n### Key Findings\\n- The study achieved an extension of the Llama-3-8B-Instruct model's context length from **8,000 tokens to 80,000 tokens**.\\n- This enhancement allows the model to better process and comprehend significantly longer text passages.\\n- Superior performance was observed across various evaluation metrics including NIHS, topic retrieval tasks, and long-context language understanding, while the model retained its effectiveness with shorter contexts.\\n\\n### Methodology\\n- The researchers implemented **Quantized Low-Rank Adaptation (QLoRA)** for the fine-tuning process.\\n- The training was executed in **just 8 hours** utilizing a robust **8xA800 (80G) GPU machine**.\\n- QLoRA enabled efficient updates to model parameters by quantizing the pre-trained model to **4-bit precision**, which significantly reduced memory usage during training.\\n\\nFor further details, please refer to the original source or linked articles:\\n- [DEV Community](https://dev.to/aimodels-fyi/extending-llama-3s-context-ten-fold-overnight-4340)\\n- [AIModels.fyi](https://www.aimodels.fyi/papers/arxiv/extending-llama-3s-context-ten-fold-overnight)\\n- [Hugging Face](https://huggingface.co/papers/2404.19553)\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"The outline has been successfully created and saved as 'post_outline.md'. It captures all the key points from the research findings on extending Llama-3's context capabilities.\", additional_kwargs={}, response_metadata={}, name='AgnosticNoteTaker'), HumanMessage(content=\"The content has been successfully written and saved to 'draft_social_post.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticDocWriter'), HumanMessage(content=\"The document 'draft_social_post.md' has been edited for improved readability and correctness. Here’s the final version:\\n\\n# Extending Llama-3's Context Tenfold Overnight\\n\\n**Authors:** Not specified  \\n\\n**Publication Date:** April 30, 2024  \\n\\n## Key Findings  \\n\\n- The study achieved an extension of the Llama-3-8B-Instruct model's context length from **8,000 tokens to 80,000 tokens**.  \\n\\n- This enhancement allows the model to better process and comprehend significantly longer text passages.  \\n\\n- Superior performance was observed across various evaluation metrics, including NIHS, topic retrieval tasks, and long-context language understanding, while the model retained its effectiveness with shorter contexts.  \\n\\n## Methodology  \\n\\n- The researchers implemented **Quantized Low-Rank Adaptation (QLoRA)** for the fine-tuning process.  \\n\\n- The training was executed in **just 8 hours** utilizing a robust **8xA800 (80G) GPU machine**.  \\n\\n- QLoRA enabled efficient updates to model parameters by quantizing the pre-trained model to **4-bit precision**, which significantly reduced memory usage during training.  \\n\\nFor further details, please refer to the original sources:  \\n\\n- [DEV Community](https://dev.to/aimodels-fyi/extending-llama-3s-context-ten-fold-overnight-4340)  \\n\\n- [AIModels.fyi](https://www.aimodels.fyi/papers/arxiv/extending-llama-3s-context-ten-fold-overnight)  \\n\\n- [Hugging Face](https://huggingface.co/papers/2404.19553)  \\n\\nIf you need any further changes, feel free to ask!\", additional_kwargs={}, response_metadata={}, name='AgnosticCopyEditor')]}}\n",
      "---\n",
      "\n",
      "--- MOVING TO: Verification team ---\n",
      "\n",
      "{'supervisor': {'next': 'Verification team'}}\n",
      "---\n",
      "Completed: Verification team\n",
      "{'Verification team': {'messages': [HumanMessage(content=\"The document 'draft_social_post.md' has been edited for improved readability and correctness. Here’s the final version:\\n\\n# Extending Llama-3's Context Tenfold Overnight\\n\\n**Authors:** Not specified  \\n\\n**Publication Date:** April 30, 2024  \\n\\n## Key Findings  \\n\\n- The study achieved an extension of the Llama-3-8B-Instruct model's context length from **8,000 tokens to 80,000 tokens**.  \\n\\n- This enhancement allows the model to better process and comprehend significantly longer text passages.  \\n\\n- Superior performance was observed across various evaluation metrics, including NIHS, topic retrieval tasks, and long-context language understanding, while the model retained its effectiveness with shorter contexts.  \\n\\n## Methodology  \\n\\n- The researchers implemented **Quantized Low-Rank Adaptation (QLoRA)** for the fine-tuning process.  \\n\\n- The training was executed in **just 8 hours** utilizing a robust **8xA800 (80G) GPU machine**.  \\n\\n- QLoRA enabled efficient updates to model parameters by quantizing the pre-trained model to **4-bit precision**, which significantly reduced memory usage during training.  \\n\\nFor further details, please refer to the original sources:  \\n\\n- [DEV Community](https://dev.to/aimodels-fyi/extending-llama-3s-context-ten-fold-overnight-4340)  \\n\\n- [AIModels.fyi](https://www.aimodels.fyi/papers/arxiv/extending-llama-3s-context-ten-fold-overnight)  \\n\\n- [Hugging Face](https://huggingface.co/papers/2404.19553)  \\n\\nIf you need any further changes, feel free to ask!\", additional_kwargs={}, response_metadata={}, name='AgnosticCopyEditor'), HumanMessage(content='FACT CHECK: APPROVED', additional_kwargs={}, response_metadata={}, name='FactChecker'), HumanMessage(content=\"STYLE CHECK: ISSUES FOUND\\n\\n1. **Length**: The post is quite long for a LinkedIn update. It could be condensed to maintain reader engagement.\\n2. **Formatting**: While headings are well-structured, the use of bullet points throughout may make it less visually appealing. Consider using paragraphs for some sections.\\n3. **Tone**: The tone is factual but lacks a personal touch or a call to action, which can encourage interaction on LinkedIn. Consider adding a sentence inviting comments or sharing thoughts.\\n4. **Context**: There’s a lack of introductory context about why this research is significant or relevant to the LinkedIn audience. Adding a brief introduction could enhance understanding. \\n\\nConsider revising these aspects to improve the post's effectiveness on LinkedIn.\", additional_kwargs={}, response_metadata={}, name='StyleValidator')], 'next': 'FINISH'}}\n",
      "---\n",
      "\n",
      "--- MOVING TO: Agnostic Authoring team ---\n",
      "\n",
      "{'supervisor': {'next': 'Agnostic Authoring team'}}\n",
      "---\n",
      "Completed: Agnostic Authoring team\n",
      "{'Agnostic Authoring team': {'messages': [HumanMessage(content=\"STYLE CHECK: ISSUES FOUND\\n\\n1. **Length**: The post is quite long for a LinkedIn update. It could be condensed to maintain reader engagement.\\n2. **Formatting**: While headings are well-structured, the use of bullet points throughout may make it less visually appealing. Consider using paragraphs for some sections.\\n3. **Tone**: The tone is factual but lacks a personal touch or a call to action, which can encourage interaction on LinkedIn. Consider adding a sentence inviting comments or sharing thoughts.\\n4. **Context**: There’s a lack of introductory context about why this research is significant or relevant to the LinkedIn audience. Adding a brief introduction could enhance understanding. \\n\\nConsider revising these aspects to improve the post's effectiveness on LinkedIn.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"The outline has been successfully created and saved as 'post_outline.md'. Here are the key points captured in the outline:\\n\\n### Title: Extending Llama-3's Context Ten-Fold Overnight\\n- **Authors:** Not specified  \\n- **Publication Date:** April 30, 2024  \\n\\n### Key Findings:\\n- Extension of Llama-3-8B-Instruct model's context length from 8,000 tokens to 80,000 tokens.\\n- Improved processing and comprehension of longer text passages.\\n- Enhanced performance across various metrics: NIHS, topic retrieval tasks, long-context language understanding, while maintaining effectiveness with shorter contexts.\\n\\n### Methodology:\\n- Implementation of Quantized Low-Rank Adaptation (QLoRA) for fine-tuning.\\n- Training duration: 8 hours on an 8xA800 (80G) GPU machine.\\n- QLoRA’s use of 4-bit precision for memory efficiency during training.\\n\\n### Additional Resources:\\n- Links to original sources for further exploration. \\n\\nThis outline will guide the document writer in creating the full post.\", additional_kwargs={}, response_metadata={}, name='AgnosticNoteTaker'), HumanMessage(content=\"The revision of the social post has been completed and saved as 'draft_social_post.md'. It now includes a more engaging tone, structured content, and a call to action for reader interaction.\", additional_kwargs={}, response_metadata={}, name='AgnosticDocWriter'), HumanMessage(content=\"I have made the necessary edits to improve readability and correctness in the draft social post. The post now has a more engaging tone, structured content, and a call to action for reader interaction. You can find the final version in the file 'draft_social_post.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticCopyEditor')]}}\n",
      "---\n",
      "\n",
      "--- MOVING TO: Verification team ---\n",
      "\n",
      "{'supervisor': {'next': 'Verification team'}}\n",
      "---\n",
      "Completed: Verification team\n",
      "{'Verification team': {'messages': [HumanMessage(content=\"I have made the necessary edits to improve readability and correctness in the draft social post. The post now has a more engaging tone, structured content, and a call to action for reader interaction. You can find the final version in the file 'draft_social_post.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticCopyEditor'), HumanMessage(content='FACT CHECK: APPROVED', additional_kwargs={}, response_metadata={}, name='FactChecker'), HumanMessage(content='STYLE CHECK: APPROVED', additional_kwargs={}, response_metadata={}, name='StyleValidator')], 'next': 'FINISH'}}\n",
      "---\n",
      "\n",
      "--- WORKFLOW ATTEMPTING TO FINISH ---\n",
      "\n",
      "Step completion status: {'Research team': True, 'Agnostic Authoring team': True, 'Verification team': True}\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "---\n",
      "Graph execution finished.\n",
      "Final step completion status: {'Research team': True, 'Agnostic Authoring team': True, 'Verification team': True}\n",
      "\n",
      "Files in working directory:\n",
      "- post_outline.md\n",
      "- draft_social_post.md\n",
      "\n",
      "Final post content:\n",
      "# Extending Llama-3's Context Ten-Fold Overnight  \n",
      "\n",
      "We are excited to share groundbreaking findings from recent research that significantly enhance the capabilities of the Llama-3-8B-Instruct model. This advancement extends the model's context length from **8,000 tokens to an unprecedented 80,000 tokens**, enabling improved processing and comprehension of longer text passages, which is crucial in today’s fast-paced information environment.  \n",
      "\n",
      "## Key Findings  \n",
      "- **Extended Context Length**: The model can now efficiently handle a context of up to 80,000 tokens.  \n",
      "- **Improved Comprehension**: Enhanced performance has been noted in various metrics, including NIHS (National Institutes of Health Scale) and topic retrieval tasks, while demonstrating robust understanding of long-context language.  \n",
      "- **Consistency with Shorter Contexts**: Remarkably, Llama-3 maintains its effectiveness even with shorter contexts, making it versatile for a wide range of applications.  \n",
      "\n",
      "## Methodology  \n",
      "The team utilized **Quantized Low-Rank Adaptation (QLoRA)** for fine-tuning the model. This innovative approach enabled training to be conducted efficiently:  \n",
      "- **Training Duration**: The fine-tuning took just **8 hours** on an 8xA800 (80G) GPU machine.  \n",
      "- **Memory Efficiency**: By employing 4-bit precision during training, QLoRA ensured optimized memory utilization without sacrificing performance.  \n",
      "\n",
      "These advancements not only push the boundaries of what Llama-3 can achieve but also set a new standard for long-context language models.  \n",
      "\n",
      "## Additional Resources  \n",
      "For those interested in diving deeper into the research, please check out the following resources:  \n",
      "- [Link to Original Research](#)  \n",
      "- [Further Reading](#)  \n",
      "\n",
      "We invite you to share your thoughts on these developments. How do you see the extension of context length impacting your work or industry? Join the conversation in the comments!  \n",
      "\n",
      "---  \n",
      "\n",
      "Looking forward to engaging discussions and insights!\n"
     ]
    }
   ],
   "source": [
    "# LinkedIn version\n",
    "result1 = run_workflow(\"Extending Llama-3's Context Ten-Fold Overnight\", platform=\"LinkedIn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using working directory: content/data/9a54acd1\n",
      "Starting graph execution...\n",
      "\n",
      "--- MOVING TO: Research team ---\n",
      "\n",
      "{'supervisor': {'next': 'Research team'}}\n",
      "---\n",
      "Completed: Research team\n",
      "{'Research team': {'messages': [HumanMessage(content=\"Write a post about the paper 'Extending Llama-3's Context Ten-Fold Overnight'. Use the standard process: Research, create a neutral draft, and then verify it for factual accuracy and suitability for the Twitter platform. Save the final post.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"# Paper Information\\n\\n## Title\\nExtending Llama-3's Context Ten-Fold Overnight\\n\\n## Authors\\n(Not specified in the search results)\\n\\n## Publication Date\\nApril 30, 2024\\n\\n## Key Findings\\n- The study successfully extended the context length of the Llama-3-8B-Instruct model from 8,000 tokens to 80,000 tokens.\\n- The new configuration allows the model to process and understand significantly larger pieces of text, enhancing its usability in long-context applications.\\n- The training process was highly efficient, taking only 8 hours on a single machine equipped with a powerful GPU setup (8xA800, 80G).\\n\\n## Methodology\\n- The researchers applied Quantized Low-Rank Adaptation (QLoRA) for fine-tuning the model.\\n- QLoRA is a method that reduces the memory requirements during fine-tuning by quantizing the model to 4-bit precision, thus allowing for large models to be trained on smaller GPU configurations.\\n- A synthetic dataset of long-context examples was generated using GPT-4 to train the model effectively.\\n\\n## Additional Notes\\nThe resulting model demonstrated improved performance across a range of evaluation tasks, including understanding long contexts, while also maintaining its original capabilities when dealing with short contexts.\\n\\n## References\\n- [DEV Community Article](https://dev.to/aimodels-fyi/extending-llama-3s-context-ten-fold-overnight)\\n- [AIModels.fyi Review](https://www.aimodels.fyi/papers/arxiv/extending-llama-3s-context-ten-fold-overnight)\", additional_kwargs={}, response_metadata={}, name='Search'), HumanMessage(content='Here are the structured research findings for the paper titled \"Extending Llama-3\\'s Context Ten-Fold Overnight\":\\n\\n### Title\\nExtending Llama-3\\'s Context Ten-Fold Overnight\\n\\n### Publication Date\\nApril 30, 2024\\n\\n### Key Findings\\n- The study successfully extended the context length of the Llama-3-8B-Instruct model from 8,000 tokens to 80,000 tokens.\\n- This increase in context length allows the model to process and understand significantly larger pieces of text, enhancing its usability in long-context applications.\\n- The training process was efficient, taking only 8 hours on a single machine equipped with a powerful GPU setup (8xA800, 80G).\\n\\n### Methodology\\n- The researchers employed Quantized Low-Rank Adaptation (QLoRA) for fine-tuning the model.\\n- QLoRA minimizes memory requirements during fine-tuning by quantizing the model to 4-bit precision, enabling large models to be trained on smaller GPU configurations.\\n- A synthetic dataset of long-context examples was generated using GPT-4 to train the model effectively.\\n\\n### Performance\\n- The resulting model demonstrated improved performance across a range of evaluation tasks, particularly in understanding long contexts, while retaining its original capabilities for dealing with short contexts.\\n\\n### References\\n- [DEV Community Article](https://dev.to/aimodels-fyi/extending-llama-3s-context-ten-fold-overnight)\\n- [AIModels.fyi Review](https://www.aimodels.fyi/papers/arxiv/extending-llama-3s-context-ten-fold-overnight)', additional_kwargs={}, response_metadata={}, name='PaperInformationRetriever')]}}\n",
      "---\n",
      "\n",
      "--- MOVING TO: Agnostic Authoring team ---\n",
      "\n",
      "{'supervisor': {'next': 'Agnostic Authoring team'}}\n",
      "---\n",
      "Completed: Agnostic Authoring team\n",
      "{'Agnostic Authoring team': {'messages': [HumanMessage(content='Here are the structured research findings for the paper titled \"Extending Llama-3\\'s Context Ten-Fold Overnight\":\\n\\n### Title\\nExtending Llama-3\\'s Context Ten-Fold Overnight\\n\\n### Publication Date\\nApril 30, 2024\\n\\n### Key Findings\\n- The study successfully extended the context length of the Llama-3-8B-Instruct model from 8,000 tokens to 80,000 tokens.\\n- This increase in context length allows the model to process and understand significantly larger pieces of text, enhancing its usability in long-context applications.\\n- The training process was efficient, taking only 8 hours on a single machine equipped with a powerful GPU setup (8xA800, 80G).\\n\\n### Methodology\\n- The researchers employed Quantized Low-Rank Adaptation (QLoRA) for fine-tuning the model.\\n- QLoRA minimizes memory requirements during fine-tuning by quantizing the model to 4-bit precision, enabling large models to be trained on smaller GPU configurations.\\n- A synthetic dataset of long-context examples was generated using GPT-4 to train the model effectively.\\n\\n### Performance\\n- The resulting model demonstrated improved performance across a range of evaluation tasks, particularly in understanding long contexts, while retaining its original capabilities for dealing with short contexts.\\n\\n### References\\n- [DEV Community Article](https://dev.to/aimodels-fyi/extending-llama-3s-context-ten-fold-overnight)\\n- [AIModels.fyi Review](https://www.aimodels.fyi/papers/arxiv/extending-llama-3s-context-ten-fold-overnight)', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"The detailed outline has been created and saved as 'post_outline.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticNoteTaker'), HumanMessage(content=\"The social post has been created and saved as 'draft_social_post.md'. If you need any further assistance, feel free to ask!\", additional_kwargs={}, response_metadata={}, name='AgnosticDocWriter'), HumanMessage(content=\"The draft social post has been edited for grammar, spelling, and tone. You can find the improved version in the file 'draft_social_post.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticCopyEditor')]}}\n",
      "---\n",
      "\n",
      "--- MOVING TO: Verification team ---\n",
      "\n",
      "{'supervisor': {'next': 'Verification team'}}\n",
      "---\n",
      "Completed: Verification team\n",
      "{'Verification team': {'messages': [HumanMessage(content=\"The draft social post has been edited for grammar, spelling, and tone. You can find the improved version in the file 'draft_social_post.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticCopyEditor'), HumanMessage(content='FACT CHECK: APPROVED', additional_kwargs={}, response_metadata={}, name='FactChecker'), HumanMessage(content=\"STYLE CHECK: ISSUES FOUND\\n\\n1. **Length**: The post is too long for Twitter, which has a character limit of 280. \\n2. **Content Structure**: The current format includes headings and bullet points that aren't suitable for Twitter's brief style.\\n3. **Tone**: The tone is more formal and detailed, which doesn't align with the typically informal and concise Twitter communication style.\\n4. **Formatting**: The use of sections such as 'Key Findings', 'Methodology', and 'Performance' do not fit the conversational and straightforward nature of Twitter posts.\\n\\nTo improve for Twitter, I recommend summarizing the key points and adopting a more casual tone while staying within the character limit.\", additional_kwargs={}, response_metadata={}, name='StyleValidator'), HumanMessage(content=\"STYLE CHECK: ISSUES FOUND\\n\\n1. **Length**: The post is too long for Twitter, which has a character limit of 280.\\n2. **Content Structure**: The current format includes headings and bullet points that aren't suitable for Twitter's brief style.\\n3. **Tone**: The tone is more formal and detailed, which doesn't align with the typically informal and concise Twitter communication style.\\n4. **Formatting**: The use of sections such as 'Key Findings', 'Methodology', and 'Performance' does not fit the conversational and straightforward nature of Twitter posts.\\n\\nTo improve for Twitter, I recommend summarizing the key points and adopting a more casual tone while staying within the character limit.\", additional_kwargs={}, response_metadata={}, name='StyleValidator')], 'next': 'FINISH'}}\n",
      "---\n",
      "\n",
      "--- MOVING TO: Agnostic Authoring team ---\n",
      "\n",
      "{'supervisor': {'next': 'Agnostic Authoring team'}}\n",
      "---\n",
      "Completed: Agnostic Authoring team\n",
      "{'Agnostic Authoring team': {'messages': [HumanMessage(content=\"STYLE CHECK: ISSUES FOUND\\n\\n1. **Length**: The post is too long for Twitter, which has a character limit of 280.\\n2. **Content Structure**: The current format includes headings and bullet points that aren't suitable for Twitter's brief style.\\n3. **Tone**: The tone is more formal and detailed, which doesn't align with the typically informal and concise Twitter communication style.\\n4. **Formatting**: The use of sections such as 'Key Findings', 'Methodology', and 'Performance' does not fit the conversational and straightforward nature of Twitter posts.\\n\\nTo improve for Twitter, I recommend summarizing the key points and adopting a more casual tone while staying within the character limit.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"The outline for the research findings has been created and saved as 'post_outline.md'. Here are the key points included in the outline:\\n\\n1. Study expands Llama-3's context from 8,000 to 80,000 tokens.\\n2. Improved usability for long-term memory tasks.\\n3. Efficient 8-hour fine-tuning on advanced GPU.\\n4. Used QLoRA for low-memory fine-tuning.\\n5. Synthetic dataset from GPT-4 for long-context examples.\\n6. Enhanced performance on evaluating long contexts.\\n\\nThis outline will assist in formulating a concise and engaging Twitter post.\", additional_kwargs={}, response_metadata={}, name='AgnosticNoteTaker'), HumanMessage(content=\"The Twitter post has been created and saved as 'draft_social_post.md'. It summarizes the research findings in a concise and engaging manner suitable for the platform.\", additional_kwargs={}, response_metadata={}, name='AgnosticDocWriter'), HumanMessage(content=\"I have edited the draft social post for clarity and tone while ensuring it fits Twitter's character limit. Here’s the improved version:\\n\\n---\\n\\n🚀 Exciting news! Our latest study expands Llama-3's context from 8K to 80K tokens, enhancing its ability to tackle long-term memory tasks! 💡 \\n\\nWe fine-tuned it for 8 hours on advanced GPUs with QLoRA, using a synthetic dataset from GPT-4 for improved long-context examples. 📊 Results show better performance! \\n\\n#AI #Llama3 #GPT4 #MachineLearning\\n\\n--- \\n\\nThe post maintains a casual tone appropriate for Twitter.\", additional_kwargs={}, response_metadata={}, name='AgnosticCopyEditor')]}}\n",
      "---\n",
      "\n",
      "--- WORKFLOW ATTEMPTING TO FINISH ---\n",
      "\n",
      "Step completion status: {'Research team': True, 'Agnostic Authoring team': True, 'Verification team': True}\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "---\n",
      "Graph execution finished.\n",
      "Final step completion status: {'Research team': True, 'Agnostic Authoring team': True, 'Verification team': True}\n",
      "\n",
      "Files in working directory:\n",
      "- post_outline.md\n",
      "- draft_social_post.md\n",
      "\n",
      "Final post content:\n",
      "🚀 Exciting news! Our latest study expands Llama-3's context from 8K to 80K tokens, boosting its ability to handle long-term memory tasks! 💡 \n",
      "\n",
      "We fine-tuned for 8 hours on advanced GPUs using QLoRA, leveraging a synthetic dataset from GPT-4 for better long-context examples. 📊 Results show enhanced performance! \n",
      "\n",
      "#AI #Llama3 #GPT4 #MachineLearning\n"
     ]
    }
   ],
   "source": [
    "#  Twitter version\n",
    "result2 = run_workflow(\"Extending Llama-3's Context Ten-Fold Overnight\", platform=\"Twitter\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using working directory: content/data/8cdeb2ef\n",
      "Starting graph execution...\n",
      "\n",
      "--- MOVING TO: Research team ---\n",
      "\n",
      "{'supervisor': {'next': 'Research team'}}\n",
      "---\n",
      "Completed: Research team\n",
      "{'Research team': {'messages': [HumanMessage(content=\"Write a post about the paper 'Extending Llama-3's Context Ten-Fold Overnight'. Use the standard process: Research, create a neutral draft, and then verify it for factual accuracy and suitability for the Medium platform. Save the final post.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"# Extending Llama-3's Context Ten-Fold Overnight\\n\\n## Title\\nExtending Llama-3's Context Ten-Fold Overnight\\n\\n## Authors\\nNot explicitly listed in the search results.\\n\\n## Publication Date\\nApril 30, 2024\\n\\n## Key Findings\\n- The study presents a method to significantly increase the context length of the Llama-3-8B-Instruct model from 8,000 tokens to 80,000 tokens.\\n- This enhancement allows the model to process much longer text effectively.\\n- The resultant model retains its capability for short contexts while excelling in long-context tasks.\\n\\n## Methodology\\n- The researchers applied **Quantized Low-Rank Adaptation (QLoRA)** for fine-tuning the model.\\n- The training involved a small dataset of synthetic long-context examples generated by GPT-4.\\n- The fine-tuning process was completed efficiently on a single powerful GPU (specifically, an 8xA800, 80G GPU) in just **8 hours**.\", additional_kwargs={}, response_metadata={}, name='Search'), HumanMessage(content=\"## Title\\nExtending Llama-3's Context Ten-Fold Overnight\\n\\n## Authors\\nNot explicitly listed in the search results.\\n\\n## Publication Date\\nApril 30, 2024\\n\\n## Key Findings\\n- The study presents a method to significantly increase the context length of the Llama-3-8B-Instruct model from 8,000 tokens to 80,000 tokens.\\n- This enhancement allows the model to process much longer texts effectively.\\n- The resultant model retains its capability for short contexts while excelling in long-context tasks.\\n\\n## Methodology\\n- The researchers applied **Quantized Low-Rank Adaptation (QLoRA)** for fine-tuning the model.\\n- The training involved a small dataset of synthetic long-context examples generated by GPT-4.\\n- The fine-tuning process was completed efficiently on a single powerful GPU (specifically, an 8xA800, 80G GPU) in just **8 hours**.\", additional_kwargs={}, response_metadata={}, name='PaperInformationRetriever')]}}\n",
      "---\n",
      "\n",
      "--- MOVING TO: Agnostic Authoring team ---\n",
      "\n",
      "{'supervisor': {'next': 'Agnostic Authoring team'}}\n",
      "---\n",
      "Completed: Agnostic Authoring team\n",
      "{'Agnostic Authoring team': {'messages': [HumanMessage(content=\"## Title\\nExtending Llama-3's Context Ten-Fold Overnight\\n\\n## Authors\\nNot explicitly listed in the search results.\\n\\n## Publication Date\\nApril 30, 2024\\n\\n## Key Findings\\n- The study presents a method to significantly increase the context length of the Llama-3-8B-Instruct model from 8,000 tokens to 80,000 tokens.\\n- This enhancement allows the model to process much longer texts effectively.\\n- The resultant model retains its capability for short contexts while excelling in long-context tasks.\\n\\n## Methodology\\n- The researchers applied **Quantized Low-Rank Adaptation (QLoRA)** for fine-tuning the model.\\n- The training involved a small dataset of synthetic long-context examples generated by GPT-4.\\n- The fine-tuning process was completed efficiently on a single powerful GPU (specifically, an 8xA800, 80G GPU) in just **8 hours**.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"The outline has been successfully created and saved as 'post_outline.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticNoteTaker'), HumanMessage(content=\"The draft social post has been successfully created and saved as 'draft_social_post.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticDocWriter'), HumanMessage(content=\"The draft social post has been successfully edited for readability and correctness. You can review the updated document in 'draft_social_post.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticCopyEditor')]}}\n",
      "---\n",
      "\n",
      "--- MOVING TO: Verification team ---\n",
      "\n",
      "{'supervisor': {'next': 'Verification team'}}\n",
      "---\n",
      "Completed: Verification team\n",
      "{'Verification team': {'messages': [HumanMessage(content=\"The draft social post has been successfully edited for readability and correctness. You can review the updated document in 'draft_social_post.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticCopyEditor'), HumanMessage(content='FACT CHECK: APPROVED', additional_kwargs={}, response_metadata={}, name='FactChecker'), HumanMessage(content='STYLE CHECK: APPROVED', additional_kwargs={}, response_metadata={}, name='StyleValidator')], 'next': 'FINISH'}}\n",
      "---\n",
      "\n",
      "--- WORKFLOW ATTEMPTING TO FINISH ---\n",
      "\n",
      "Step completion status: {'Research team': True, 'Agnostic Authoring team': True, 'Verification team': True}\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "---\n",
      "Graph execution finished.\n",
      "Final step completion status: {'Research team': True, 'Agnostic Authoring team': True, 'Verification team': True}\n",
      "\n",
      "Files in working directory:\n",
      "- post_outline.md\n",
      "- draft_social_post.md\n",
      "\n",
      "Final post content:\n",
      "# Extending Llama-3's Context Ten-Fold Overnight\n",
      "\n",
      "**Key Findings**  \n",
      "- The study presents a method to significantly increase the context length of the Llama-3-8B-Instruct model from 8,000 tokens to **80,000 tokens**.  \n",
      "- This enhancement allows the model to process much longer texts effectively.  \n",
      "- The resultant model retains its capability for short contexts while excelling in long-context tasks.\n",
      "\n",
      "**Methodology**  \n",
      "- The researchers applied **Quantized Low-Rank Adaptation (QLoRA)** for fine-tuning the model.  \n",
      "- The training involved a small dataset of synthetic long-context examples generated by GPT-4.  \n",
      "- The fine-tuning process was completed efficiently on a single powerful GPU (specifically, an 8xA800, 80G GPU) in just **8 hours**.\n"
     ]
    }
   ],
   "source": [
    "# Medium version\n",
    "result3 = run_workflow(\"Extending Llama-3's Context Ten-Fold Overnight\", platform=\"Medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using working directory: content/data/f56625b2\n",
      "Starting graph execution...\n",
      "\n",
      "--- MOVING TO: Research team ---\n",
      "\n",
      "{'supervisor': {'next': 'Research team'}}\n",
      "---\n",
      "Completed: Research team\n",
      "{'Research team': {'messages': [HumanMessage(content=\"Write a post about the paper 'Offline Reinforcement Learning'. Use the standard process: Research, create a neutral draft, and then verify it for factual accuracy and suitability for the LinkedIn platform. Save the final post.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='## Offline Reinforcement Learning: Key Information\\n\\n### Title\\n- **\"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems\"**\\n\\n### Authors\\n- The paper does not specify unique authors in the search result. However, it is commonly attributed to a collaboration of multiple authors in the reinforcement learning community.\\n\\n### Publication Date\\n- The paper was initially submitted to arXiv on **May 5, 2020**.\\n\\n### Key Findings\\n- The paper serves as a comprehensive review and tutorial on offline reinforcement learning (RL), providing insights into its various methodologies, challenges, and open problems. It discusses the potential of offline RL to facilitate learning from previously collected data without requiring further exploration or interaction with the environment, thus addressing the exploration bottleneck associated with traditional online learning.\\n\\n### Methodology\\n- This review paper collates existing research on offline RL, categorizing core methodologies under distinct problem settings (like batch RL). It synthesizes theoretical and empirical findings, highlighting the effectiveness of different approaches and algorithms designed to operate within the offline paradigm.\\n\\nFor further details, you can access the full paper [here](https://arxiv.org/abs/2005.01643).', additional_kwargs={}, response_metadata={}, name='Search'), HumanMessage(content='### Title\\n- **\"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems\"**\\n\\n### Authors\\n- The specific authors are not mentioned in the available summary, but it is a collaborative work involving multiple contributors from the reinforcement learning field.\\n\\n### Publication Date\\n- Submitted to arXiv on **May 5, 2020**.\\n\\n### Key Findings\\n- This paper functions as a comprehensive review and tutorial on offline reinforcement learning (RL), discussing its methodologies, challenges, and open questions. It emphasizes offline RL\\'s capability to learn from previously collected datasets, eliminating the need for further exploratory actions within the environment, thereby mitigating the exploration challenges present in traditional online learning paradigms.\\n\\n### Methodology\\n- The review categorizes various approaches to offline RL, organizing them under distinct problem settings such as batch RL. It synthesizes both theoretical and empirical results, showcasing the effectiveness of different algorithms adapted to work within the offline learning framework.\\n\\n### Access\\n- For further details, you can access the full paper [here](https://arxiv.org/abs/2005.01643).', additional_kwargs={}, response_metadata={}, name='PaperInformationRetriever')]}}\n",
      "---\n",
      "\n",
      "--- MOVING TO: Agnostic Authoring team ---\n",
      "\n",
      "{'supervisor': {'next': 'Agnostic Authoring team'}}\n",
      "---\n",
      "Completed: Agnostic Authoring team\n",
      "{'Agnostic Authoring team': {'messages': [HumanMessage(content='### Title\\n- **\"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems\"**\\n\\n### Authors\\n- The specific authors are not mentioned in the available summary, but it is a collaborative work involving multiple contributors from the reinforcement learning field.\\n\\n### Publication Date\\n- Submitted to arXiv on **May 5, 2020**.\\n\\n### Key Findings\\n- This paper functions as a comprehensive review and tutorial on offline reinforcement learning (RL), discussing its methodologies, challenges, and open questions. It emphasizes offline RL\\'s capability to learn from previously collected datasets, eliminating the need for further exploratory actions within the environment, thereby mitigating the exploration challenges present in traditional online learning paradigms.\\n\\n### Methodology\\n- The review categorizes various approaches to offline RL, organizing them under distinct problem settings such as batch RL. It synthesizes both theoretical and empirical results, showcasing the effectiveness of different algorithms adapted to work within the offline learning framework.\\n\\n### Access\\n- For further details, you can access the full paper [here](https://arxiv.org/abs/2005.01643).', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"The outline has been successfully created and saved as 'post_outline.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticNoteTaker'), HumanMessage(content=\"The content has been successfully created and saved to the file named 'draft_social_post.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticDocWriter'), HumanMessage(content='The draft has been successfully edited for grammar, spelling, and tone. Here is the improved version:\\n\\n# Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems\\n\\n### Overview\\n\\nThis post discusses a comprehensive review and tutorial on offline reinforcement learning (RL) as presented in the paper titled \"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems,\" submitted to arXiv on **May 5, 2020**.\\n\\n### Key Findings\\n\\nThe paper serves as an extensive examination of offline reinforcement learning, highlighting its methodologies, challenges, and unresolved issues. Notably, offline RL can learn from pre-existing datasets without the need for further exploratory actions in the environment. This characteristic helps to alleviate the exploration difficulties found in traditional online learning approaches.\\n\\n### Methodology\\n\\nThe authors categorize various offline RL approaches into distinct problem settings, including batch RL. They provide a synthesis of both theoretical insights and empirical results that demonstrate the effectiveness of diverse algorithms tailored for offline learning.\\n\\n### Conclusion\\n\\nThe tutorial and review effectively lay the groundwork for understanding offline reinforcement learning, its potential applications, and future directions.\\n\\n### Access\\n\\nTo delve deeper into the subject, the full paper is available [here](https://arxiv.org/abs/2005.01643).', additional_kwargs={}, response_metadata={}, name='AgnosticCopyEditor')]}}\n",
      "---\n",
      "\n",
      "--- MOVING TO: Verification team ---\n",
      "\n",
      "{'supervisor': {'next': 'Verification team'}}\n",
      "---\n",
      "Completed: Verification team\n",
      "{'Verification team': {'messages': [HumanMessage(content='The draft has been successfully edited for grammar, spelling, and tone. Here is the improved version:\\n\\n# Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems\\n\\n### Overview\\n\\nThis post discusses a comprehensive review and tutorial on offline reinforcement learning (RL) as presented in the paper titled \"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems,\" submitted to arXiv on **May 5, 2020**.\\n\\n### Key Findings\\n\\nThe paper serves as an extensive examination of offline reinforcement learning, highlighting its methodologies, challenges, and unresolved issues. Notably, offline RL can learn from pre-existing datasets without the need for further exploratory actions in the environment. This characteristic helps to alleviate the exploration difficulties found in traditional online learning approaches.\\n\\n### Methodology\\n\\nThe authors categorize various offline RL approaches into distinct problem settings, including batch RL. They provide a synthesis of both theoretical insights and empirical results that demonstrate the effectiveness of diverse algorithms tailored for offline learning.\\n\\n### Conclusion\\n\\nThe tutorial and review effectively lay the groundwork for understanding offline reinforcement learning, its potential applications, and future directions.\\n\\n### Access\\n\\nTo delve deeper into the subject, the full paper is available [here](https://arxiv.org/abs/2005.01643).', additional_kwargs={}, response_metadata={}, name='AgnosticCopyEditor'), HumanMessage(content='FACT CHECK: APPROVED', additional_kwargs={}, response_metadata={}, name='FactChecker'), HumanMessage(content='STYLE CHECK: APPROVED', additional_kwargs={}, response_metadata={}, name='StyleValidator')], 'next': 'FINISH'}}\n",
      "---\n",
      "\n",
      "--- WORKFLOW ATTEMPTING TO FINISH ---\n",
      "\n",
      "Step completion status: {'Research team': True, 'Agnostic Authoring team': True, 'Verification team': True}\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "---\n",
      "Graph execution finished.\n",
      "Final step completion status: {'Research team': True, 'Agnostic Authoring team': True, 'Verification team': True}\n",
      "\n",
      "Files in working directory:\n",
      "- post_outline.md\n",
      "- draft_social_post.md\n",
      "\n",
      "Final post content:\n",
      "# Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems\n",
      "\n",
      "### Overview\n",
      "This post discusses a comprehensive review and tutorial on offline reinforcement learning (RL) as presented in the paper titled \"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems\" submitted to arXiv on **May 5, 2020**. \n",
      "\n",
      "### Key Findings\n",
      "The paper serves as an extensive examination of offline reinforcement learning, highlighting its methodologies, challenges, and unresolved issues. Notably, offline RL can learn from pre-existing datasets without the need for further exploratory actions in the environment. This characteristic helps to alleviate the exploration difficulties found in traditional online learning approaches.\n",
      "\n",
      "### Methodology\n",
      "The authors categorize various offline RL approaches into distinct problem settings, including batch RL. They provide a synthesis of both theoretical insights and empirical results that demonstrate the effectiveness of diverse algorithms tailored for offline learning.\n",
      "\n",
      "### Conclusion\n",
      "The tutorial and review effectively lay the groundwork for understanding offline reinforcement learning, its potential applications, and future directions. \n",
      "\n",
      "### Access\n",
      "To delve deeper into the subject, the full paper is available [here](https://arxiv.org/abs/2005.01643).\n"
     ]
    }
   ],
   "source": [
    "result4  = run_workflow(\"Offline Reinforcement Learning\", platform=\"LinkedIn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using working directory: content/data/d28bb8a0\n",
      "Starting graph execution...\n",
      "\n",
      "--- MOVING TO: Research team ---\n",
      "\n",
      "{'supervisor': {'next': 'Research team'}}\n",
      "---\n",
      "Completed: Research team\n",
      "{'Research team': {'messages': [HumanMessage(content=\"Write a post about the paper 'Offline Reinforcement Learning'. Use the standard process: Research, create a neutral draft, and then verify it for factual accuracy and suitability for the Tweeter platform. Save the final post.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"# Paper Information\\n\\n### Title\\n**Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems**\\n\\n### Authors\\nThe paper does not list specific authors in the extracted data. It's advisable to check the original paper for a complete authorship list.\\n\\n### Publication Date\\nThe submission history indicates the paper can be found on arXiv, with revisions. The original version was submitted on May 5, 2020.\\n\\n### Key Findings\\n- **Concept**: Offline reinforcement learning (RL) allows learning from a fixed batch of data without further interaction with the environment.\\n- **Challenges**: Most offline RL algorithms struggle with value estimation errors due to out-of-distribution actions, leading to complex modifications in standard algorithms.\\n- **Methodology Insight**: The authors review existing methods and propose perspectives on unresolved challenges in the field of offline RL.\\n\\n### Methodology\\nThe methodology focuses on analyzing various offline RL approaches through a review and tutorial format, discussing the implications of batch data, the modification of existing algorithms, and the need for adaptation in their application to ensure robustness and performance.\\n\\n### Source\\nFor more detailed insights, the paper can be accessed [here](https://arxiv.org/abs/2005.01643).\", additional_kwargs={}, response_metadata={}, name='Search'), HumanMessage(content='### Paper Information\\n\\n**Title**: Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems\\n\\n**Authors**: Not specified in the extracted data.\\n\\n**Publication Date**: Original version submitted on May 5, 2020, available on arXiv.\\n\\n### Key Findings\\n\\n- **Concept**: Offline reinforcement learning (RL) enables learning from a fixed batch of data without requiring further interaction with the environment.\\n- **Challenges**: A significant challenge for most offline RL algorithms is the value estimation errors that arise due to actions being out-of-distribution, which necessitates sophisticated modifications of standard algorithms.\\n- **Methodology Insight**: The paper reviews existing methods in the field and offers perspectives on unresolved challenges in offline RL.\\n\\n### Methodology\\n\\nThe methodology emphasizes a review and tutorial format, exploring various offline RL approaches, addressing the implications of batch data, algorithm modifications, and the necessary adaptations for enhancing robustness and performance.\\n\\n### Source\\n\\nFor further details, the paper can be accessed [here](https://arxiv.org/abs/2005.01643).', additional_kwargs={}, response_metadata={}, name='PaperInformationRetriever')]}}\n",
      "---\n",
      "\n",
      "--- MOVING TO: Agnostic Authoring team ---\n",
      "\n",
      "{'supervisor': {'next': 'Agnostic Authoring team'}}\n",
      "---\n",
      "Completed: Agnostic Authoring team\n",
      "{'Agnostic Authoring team': {'messages': [HumanMessage(content='### Paper Information\\n\\n**Title**: Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems\\n\\n**Authors**: Not specified in the extracted data.\\n\\n**Publication Date**: Original version submitted on May 5, 2020, available on arXiv.\\n\\n### Key Findings\\n\\n- **Concept**: Offline reinforcement learning (RL) enables learning from a fixed batch of data without requiring further interaction with the environment.\\n- **Challenges**: A significant challenge for most offline RL algorithms is the value estimation errors that arise due to actions being out-of-distribution, which necessitates sophisticated modifications of standard algorithms.\\n- **Methodology Insight**: The paper reviews existing methods in the field and offers perspectives on unresolved challenges in offline RL.\\n\\n### Methodology\\n\\nThe methodology emphasizes a review and tutorial format, exploring various offline RL approaches, addressing the implications of batch data, algorithm modifications, and the necessary adaptations for enhancing robustness and performance.\\n\\n### Source\\n\\nFor further details, the paper can be accessed [here](https://arxiv.org/abs/2005.01643).', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"The detailed outline has been successfully created and saved as 'post_outline.md'. You can now use it to draft the full post on the specified paper.\", additional_kwargs={}, response_metadata={}, name='AgnosticNoteTaker'), HumanMessage(content=\"The draft social post has been successfully created and saved as 'draft_social_post.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticDocWriter'), HumanMessage(content=\"The draft social post has been edited for grammar, spelling, and tone. Here is the revised version:\\n\\n# Exploring Offline Reinforcement Learning: Insights and Challenges\\n\\nOffline reinforcement learning (RL) has emerged as a fascinating area of study, allowing for the utilization of previously collected data to enhance learning capabilities without further interaction with the environment. This advancement opens up innovative avenues for applying machine learning in various scenarios where real-time interaction is limited or impractical.\\n\\n### Key Findings from Recent Research\\n\\nA recent paper titled *Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems* discusses several critical aspects of offline RL. Here are the highlights:\\n\\n- **Conceptual Framework**: Offline RL is designed to learn from a fixed batch of data gathered from past interactions, contrasting with traditional RL, which typically requires ongoing interaction with the environment.\\n\\n- **Challenges Identified**: A primary challenge within offline RL is the occurrence of value estimation errors. These errors often arise when the actions taken are out-of-distribution relative to the data used for training, prompting a need for advanced modifications to conventional algorithms.\\n\\n- **Methodological Review**: The paper provides a thorough review of the existing methodologies within offline RL, shedding light on how these approaches address the implications of using batch data. It discusses various algorithmic adjustments required for enhancing robustness and performance in offline settings.\\n\\n### Perspectives on Open Problems\\n\\nIn addition to reviewing established methods, the paper also articulates perspectives on unresolved challenges within the offline RL domain. Tackling these issues is essential for advancing the field and improving the applicability of offline RL techniques in real-world scenarios.\\n\\nIf you're interested in delving deeper into the intricacies of offline reinforcement learning and obtaining comprehensive knowledge on its current state and future directions, check out the full paper [here](https://arxiv.org/abs/2005.01643).\", additional_kwargs={}, response_metadata={}, name='AgnosticCopyEditor')]}}\n",
      "---\n",
      "\n",
      "--- MOVING TO: Verification team ---\n",
      "\n",
      "{'supervisor': {'next': 'Verification team'}}\n",
      "---\n",
      "Completed: Verification team\n",
      "{'Verification team': {'messages': [HumanMessage(content=\"The draft social post has been edited for grammar, spelling, and tone. Here is the revised version:\\n\\n# Exploring Offline Reinforcement Learning: Insights and Challenges\\n\\nOffline reinforcement learning (RL) has emerged as a fascinating area of study, allowing for the utilization of previously collected data to enhance learning capabilities without further interaction with the environment. This advancement opens up innovative avenues for applying machine learning in various scenarios where real-time interaction is limited or impractical.\\n\\n### Key Findings from Recent Research\\n\\nA recent paper titled *Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems* discusses several critical aspects of offline RL. Here are the highlights:\\n\\n- **Conceptual Framework**: Offline RL is designed to learn from a fixed batch of data gathered from past interactions, contrasting with traditional RL, which typically requires ongoing interaction with the environment.\\n\\n- **Challenges Identified**: A primary challenge within offline RL is the occurrence of value estimation errors. These errors often arise when the actions taken are out-of-distribution relative to the data used for training, prompting a need for advanced modifications to conventional algorithms.\\n\\n- **Methodological Review**: The paper provides a thorough review of the existing methodologies within offline RL, shedding light on how these approaches address the implications of using batch data. It discusses various algorithmic adjustments required for enhancing robustness and performance in offline settings.\\n\\n### Perspectives on Open Problems\\n\\nIn addition to reviewing established methods, the paper also articulates perspectives on unresolved challenges within the offline RL domain. Tackling these issues is essential for advancing the field and improving the applicability of offline RL techniques in real-world scenarios.\\n\\nIf you're interested in delving deeper into the intricacies of offline reinforcement learning and obtaining comprehensive knowledge on its current state and future directions, check out the full paper [here](https://arxiv.org/abs/2005.01643).\", additional_kwargs={}, response_metadata={}, name='AgnosticCopyEditor'), HumanMessage(content='FACT CHECK: APPROVED', additional_kwargs={}, response_metadata={}, name='FactChecker'), HumanMessage(content='STYLE CHECK: ISSUES FOUND\\n\\n1. **Length**: Twitter has a character limit of 280 characters, and the provided text far exceeds this limit.\\n2. **Format**: The use of headers and bullet points is not suitable for Twitter, which typically favors concise messages rather than structured formats.\\n3. **Link Placement**: The link to the full paper should be integrated more fluidly with the text, as Twitter often has a limit on how links can be used without taking up excessive character space.\\n4. **Engagement**: The tone is informative but lacks a compelling call to action or hashtags that could increase engagement on social media.', additional_kwargs={}, response_metadata={}, name='StyleValidator'), HumanMessage(content='STYLE CHECK: ISSUES FOUND\\n\\n1. **Length**: Twitter has a character limit of 280 characters, and the provided text far exceeds this limit.\\n2. **Format**: The use of headers and bullet points is not suitable for Twitter, which typically favors concise messages rather than structured formats.\\n3. **Link Placement**: The link to the full paper should be integrated more fluidly with the text, as Twitter often has a limit on how links can be used without taking up excessive character space.\\n4. **Engagement**: The tone is informative but lacks a compelling call to action or hashtags that could increase engagement on social media.', additional_kwargs={}, response_metadata={}, name='StyleValidator')], 'next': 'FINISH'}}\n",
      "---\n",
      "\n",
      "--- MOVING TO: Agnostic Authoring team ---\n",
      "\n",
      "{'supervisor': {'next': 'Agnostic Authoring team'}}\n",
      "---\n",
      "Completed: Agnostic Authoring team\n",
      "{'Agnostic Authoring team': {'messages': [HumanMessage(content='STYLE CHECK: ISSUES FOUND\\n\\n1. **Length**: Twitter has a character limit of 280 characters, and the provided text far exceeds this limit.\\n2. **Format**: The use of headers and bullet points is not suitable for Twitter, which typically favors concise messages rather than structured formats.\\n3. **Link Placement**: The link to the full paper should be integrated more fluidly with the text, as Twitter often has a limit on how links can be used without taking up excessive character space.\\n4. **Engagement**: The tone is informative but lacks a compelling call to action or hashtags that could increase engagement on social media.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"The outline has been successfully created and saved as 'post_outline.md'. It captures the key points from the draft social post about offline reinforcement learning.\", additional_kwargs={}, response_metadata={}, name='AgnosticNoteTaker'), HumanMessage(content=\"The draft social post has been successfully updated to be more suitable for Twitter. Here’s the final version:\\n\\n---\\n\\n🚀 Dive into Offline Reinforcement Learning! A recent paper reviews how we can learn from fixed datasets, tackling challenges like value estimation errors in algorithm applications. Explore innovative solutions for real-world scenarios! Read more: https://arxiv.org/abs/2005.01643 #ReinforcementLearning #MachineLearning\\n\\n---\\n\\nThe content is concise, integrates the link fluidly, and includes relevant hashtags to enhance engagement. The final draft has been saved as 'draft_social_post.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticDocWriter'), HumanMessage(content=\"The draft social post has been successfully edited and saved. You can find the final version of the post in 'draft_social_post.md'.\", additional_kwargs={}, response_metadata={}, name='AgnosticCopyEditor')]}}\n",
      "---\n",
      "\n",
      "--- WORKFLOW ATTEMPTING TO FINISH ---\n",
      "\n",
      "Step completion status: {'Research team': True, 'Agnostic Authoring team': True, 'Verification team': True}\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "---\n",
      "Graph execution finished.\n",
      "Final step completion status: {'Research team': True, 'Agnostic Authoring team': True, 'Verification team': True}\n",
      "\n",
      "Files in working directory:\n",
      "- post_outline.md\n",
      "- draft_social_post.md\n",
      "\n",
      "Final post content:\n",
      "🚀 Dive into Offline Reinforcement Learning! A recent paper reviews how we can learn from fixed datasets, tackling challenges like value estimation errors in algorithm applications. Explore innovative solutions for real-world scenarios! Read more: https://arxiv.org/abs/2005.01643 #ReinforcementLearning #MachineLearning\n"
     ]
    }
   ],
   "source": [
    "result5  = run_workflow(\"Offline Reinforcement Learning\", platform=\"Tweeter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
