1. Study expands Llama-3's context from 8,000 to 80,000 tokens.
2. Improved usability for long-term memory tasks.
3. Efficient 8-hour fine-tuning on advanced GPU.
4. Used QLoRA for low-memory fine-tuning.
5. Synthetic dataset from GPT-4 for long-context examples.
6. Enhanced performance on evaluating long contexts.
