# Extending Llama-3's Context Ten-Fold Overnight

**Key Findings**  
- The study presents a method to significantly increase the context length of the Llama-3-8B-Instruct model from 8,000 tokens to **80,000 tokens**.  
- This enhancement allows the model to process much longer texts effectively.  
- The resultant model retains its capability for short contexts while excelling in long-context tasks.

**Methodology**  
- The researchers applied **Quantized Low-Rank Adaptation (QLoRA)** for fine-tuning the model.  
- The training involved a small dataset of synthetic long-context examples generated by GPT-4.  
- The fine-tuning process was completed efficiently on a single powerful GPU (specifically, an 8xA800, 80G GPU) in just **8 hours**.